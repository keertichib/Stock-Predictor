{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "3ae6752b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "76b1b022",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('AAPL.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "7e6dcbe7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>symbol</th>\n",
       "      <th>date</th>\n",
       "      <th>close</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>open</th>\n",
       "      <th>volume</th>\n",
       "      <th>adjClose</th>\n",
       "      <th>adjHigh</th>\n",
       "      <th>adjLow</th>\n",
       "      <th>adjOpen</th>\n",
       "      <th>adjVolume</th>\n",
       "      <th>divCash</th>\n",
       "      <th>splitFactor</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>2015-05-27 00:00:00+00:00</td>\n",
       "      <td>132.045</td>\n",
       "      <td>132.260</td>\n",
       "      <td>130.05</td>\n",
       "      <td>130.34</td>\n",
       "      <td>45833246</td>\n",
       "      <td>121.682558</td>\n",
       "      <td>121.880685</td>\n",
       "      <td>119.844118</td>\n",
       "      <td>120.111360</td>\n",
       "      <td>45833246</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>2015-05-28 00:00:00+00:00</td>\n",
       "      <td>131.780</td>\n",
       "      <td>131.950</td>\n",
       "      <td>131.10</td>\n",
       "      <td>131.86</td>\n",
       "      <td>30733309</td>\n",
       "      <td>121.438354</td>\n",
       "      <td>121.595013</td>\n",
       "      <td>120.811718</td>\n",
       "      <td>121.512076</td>\n",
       "      <td>30733309</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>2015-05-29 00:00:00+00:00</td>\n",
       "      <td>130.280</td>\n",
       "      <td>131.450</td>\n",
       "      <td>129.90</td>\n",
       "      <td>131.23</td>\n",
       "      <td>50884452</td>\n",
       "      <td>120.056069</td>\n",
       "      <td>121.134251</td>\n",
       "      <td>119.705890</td>\n",
       "      <td>120.931516</td>\n",
       "      <td>50884452</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>2015-06-01 00:00:00+00:00</td>\n",
       "      <td>130.535</td>\n",
       "      <td>131.390</td>\n",
       "      <td>130.05</td>\n",
       "      <td>131.20</td>\n",
       "      <td>32112797</td>\n",
       "      <td>120.291057</td>\n",
       "      <td>121.078960</td>\n",
       "      <td>119.844118</td>\n",
       "      <td>120.903870</td>\n",
       "      <td>32112797</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>2015-06-02 00:00:00+00:00</td>\n",
       "      <td>129.960</td>\n",
       "      <td>130.655</td>\n",
       "      <td>129.32</td>\n",
       "      <td>129.86</td>\n",
       "      <td>33667627</td>\n",
       "      <td>119.761181</td>\n",
       "      <td>120.401640</td>\n",
       "      <td>119.171406</td>\n",
       "      <td>119.669029</td>\n",
       "      <td>33667627</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0 symbol                       date    close     high     low  \\\n",
       "0           0   AAPL  2015-05-27 00:00:00+00:00  132.045  132.260  130.05   \n",
       "1           1   AAPL  2015-05-28 00:00:00+00:00  131.780  131.950  131.10   \n",
       "2           2   AAPL  2015-05-29 00:00:00+00:00  130.280  131.450  129.90   \n",
       "3           3   AAPL  2015-06-01 00:00:00+00:00  130.535  131.390  130.05   \n",
       "4           4   AAPL  2015-06-02 00:00:00+00:00  129.960  130.655  129.32   \n",
       "\n",
       "     open    volume    adjClose     adjHigh      adjLow     adjOpen  \\\n",
       "0  130.34  45833246  121.682558  121.880685  119.844118  120.111360   \n",
       "1  131.86  30733309  121.438354  121.595013  120.811718  121.512076   \n",
       "2  131.23  50884452  120.056069  121.134251  119.705890  120.931516   \n",
       "3  131.20  32112797  120.291057  121.078960  119.844118  120.903870   \n",
       "4  129.86  33667627  119.761181  120.401640  119.171406  119.669029   \n",
       "\n",
       "   adjVolume  divCash  splitFactor  \n",
       "0   45833246      0.0          1.0  \n",
       "1   30733309      0.0          1.0  \n",
       "2   50884452      0.0          1.0  \n",
       "3   32112797      0.0          1.0  \n",
       "4   33667627      0.0          1.0  "
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "b1927ea1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>symbol</th>\n",
       "      <th>date</th>\n",
       "      <th>close</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>open</th>\n",
       "      <th>volume</th>\n",
       "      <th>adjClose</th>\n",
       "      <th>adjHigh</th>\n",
       "      <th>adjLow</th>\n",
       "      <th>adjOpen</th>\n",
       "      <th>adjVolume</th>\n",
       "      <th>divCash</th>\n",
       "      <th>splitFactor</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1253</th>\n",
       "      <td>1253</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>2020-05-18 00:00:00+00:00</td>\n",
       "      <td>314.96</td>\n",
       "      <td>316.50</td>\n",
       "      <td>310.3241</td>\n",
       "      <td>313.17</td>\n",
       "      <td>33843125</td>\n",
       "      <td>314.96</td>\n",
       "      <td>316.50</td>\n",
       "      <td>310.3241</td>\n",
       "      <td>313.17</td>\n",
       "      <td>33843125</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1254</th>\n",
       "      <td>1254</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>2020-05-19 00:00:00+00:00</td>\n",
       "      <td>313.14</td>\n",
       "      <td>318.52</td>\n",
       "      <td>313.0100</td>\n",
       "      <td>315.03</td>\n",
       "      <td>25432385</td>\n",
       "      <td>313.14</td>\n",
       "      <td>318.52</td>\n",
       "      <td>313.0100</td>\n",
       "      <td>315.03</td>\n",
       "      <td>25432385</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1255</th>\n",
       "      <td>1255</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>2020-05-20 00:00:00+00:00</td>\n",
       "      <td>319.23</td>\n",
       "      <td>319.52</td>\n",
       "      <td>316.2000</td>\n",
       "      <td>316.68</td>\n",
       "      <td>27876215</td>\n",
       "      <td>319.23</td>\n",
       "      <td>319.52</td>\n",
       "      <td>316.2000</td>\n",
       "      <td>316.68</td>\n",
       "      <td>27876215</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1256</th>\n",
       "      <td>1256</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>2020-05-21 00:00:00+00:00</td>\n",
       "      <td>316.85</td>\n",
       "      <td>320.89</td>\n",
       "      <td>315.8700</td>\n",
       "      <td>318.66</td>\n",
       "      <td>25672211</td>\n",
       "      <td>316.85</td>\n",
       "      <td>320.89</td>\n",
       "      <td>315.8700</td>\n",
       "      <td>318.66</td>\n",
       "      <td>25672211</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1257</th>\n",
       "      <td>1257</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>2020-05-22 00:00:00+00:00</td>\n",
       "      <td>318.89</td>\n",
       "      <td>319.23</td>\n",
       "      <td>315.3500</td>\n",
       "      <td>315.77</td>\n",
       "      <td>20450754</td>\n",
       "      <td>318.89</td>\n",
       "      <td>319.23</td>\n",
       "      <td>315.3500</td>\n",
       "      <td>315.77</td>\n",
       "      <td>20450754</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0 symbol                       date   close    high       low  \\\n",
       "1253        1253   AAPL  2020-05-18 00:00:00+00:00  314.96  316.50  310.3241   \n",
       "1254        1254   AAPL  2020-05-19 00:00:00+00:00  313.14  318.52  313.0100   \n",
       "1255        1255   AAPL  2020-05-20 00:00:00+00:00  319.23  319.52  316.2000   \n",
       "1256        1256   AAPL  2020-05-21 00:00:00+00:00  316.85  320.89  315.8700   \n",
       "1257        1257   AAPL  2020-05-22 00:00:00+00:00  318.89  319.23  315.3500   \n",
       "\n",
       "        open    volume  adjClose  adjHigh    adjLow  adjOpen  adjVolume  \\\n",
       "1253  313.17  33843125    314.96   316.50  310.3241   313.17   33843125   \n",
       "1254  315.03  25432385    313.14   318.52  313.0100   315.03   25432385   \n",
       "1255  316.68  27876215    319.23   319.52  316.2000   316.68   27876215   \n",
       "1256  318.66  25672211    316.85   320.89  315.8700   318.66   25672211   \n",
       "1257  315.77  20450754    318.89   319.23  315.3500   315.77   20450754   \n",
       "\n",
       "      divCash  splitFactor  \n",
       "1253      0.0          1.0  \n",
       "1254      0.0          1.0  \n",
       "1255      0.0          1.0  \n",
       "1256      0.0          1.0  \n",
       "1257      0.0          1.0  "
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "62023039",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1=df.reset_index()['close']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "869bbc0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       132.045\n",
       "1       131.780\n",
       "2       130.280\n",
       "3       130.535\n",
       "4       129.960\n",
       "         ...   \n",
       "1253    314.960\n",
       "1254    313.140\n",
       "1255    319.230\n",
       "1256    316.850\n",
       "1257    318.890\n",
       "Name: close, Length: 1258, dtype: float64"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "d3cf8389",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1d0ad1e0dc0>]"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD4CAYAAAAXUaZHAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjYuMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy89olMNAAAACXBIWXMAAAsTAAALEwEAmpwYAAA18UlEQVR4nO3deXhU1fnA8e/JvickBAgJEHYEVDaRRRR3FCtqrUtri/tubbXtT6utS2uLXbRqa61W61LrUrXu0KLihggCArITQgJhy0L2ZZJJzu+Pe2dyZ0smyWRmMnk/z8PDnXPvzLyTSd45c+6571Faa4QQQkSWqFAHIIQQIvAkuQshRASS5C6EEBFIkrsQQkQgSe5CCBGBYkIdAMDAgQN1fn5+qMMQQog+Zd26deVa62xv+8Iiuefn57N27dpQhyGEEH2KUqrY1z4ZlhFCiAgkyV0IISKQJHchhIhAktyFECICSXIXQogIJMldCCEikCR3IYSIQJLchRCiGz7ZWUZxRX2ow/ApLC5iEkKIvmbxM2sAKFqyMMSReCc9dyGE6IKmllbufOMb5+1wXfBIeu5CCOGnL3aX892nVru0bdhXxdThA0IUkW/ScxdCCD99tafSo+38x78IQSSdk+QuhBB+ilKhjsB/ktyFEMJP9rbwHF/3RpK7EEL44Z9fFvPIh7sC9nirCyvIv+M9lm0+FLDHtJLkLoQQnaiz2bn7zc0ubb/79jEARHdzrObml74G4L9bJLkLIUTQaa2ZfM9/PdrnT8jm4hnDGJgS163HbTOHeAalxfcoPl8kuQshRAds9jav7fEx0STGRdPY3Nrlx9xUUsW4wakA3HzymB7F54vMcxdCiA40+Eje8TFRJMRG09TiPfn7suVANef+eaXzdmpCbI/i80V67kII0YF6m91re1x0FImx0TS3tmFv9T/Bl9bYAhVahyS5CyFEBxpbvPfco6IUCbFGCvU1dONNm6VcQW5GYs+C64AkdyGE6ICvYRmA2Ggjhdpb/Z//XtPU4ty+Ym5+t+PqjCR3IYTowK7DtQD84/LjPPbFxhgptLkLwzLVDe3JPS6m91KwJHchhOhAUUU90VGK+eOzPfbFRRtz3Fu6ktwb28fw9x1p6HmAPshsGSGE6EC9rZWU+BiUUmy85wwOVjc6e9+OYZmuJff2nrtjOmRvkOQuhBAdqG2ykxJvpMr0xFjSE9unLnY3uSfFRfP3xTOYPSorsMFaSHIXQogO1NvsJMdHe93nSO7Ndv9PqFY3NjMiK5k5owcGJD5fZMxdCCE6UN9sJzneez84LqY7Y+4tpCf2fr+60+SulEpQSq1RSm1USm1RSt1nto9USq1WShUopV5RSsWZ7fHm7QJzf34vvwYhhOg1dbb2YRl3jp77nW98Q/4d7/n1eEZy752rUq386bnbgFO01scCU4AFSqlZwIPAw1rrMUAlcJV5/FVApdn+sHmcEEL0OVprlzF3d47kvvVgjfP4zlQ3tpCR2L1iY13RaXLXhjrzZqz5TwOnAK+Z7c8B55nbi8zbmPtPVUr1ofVLhBDCsPDRzykorcNXznYkdwd/5rtXN7aQnhQePXeUUtFKqQ1AKbAc2A1Uaa0dEzZLgFxzOxfYB2DurwY8Tgkrpa5VSq1VSq0tKyvr0YsQQoje4OiRL/NRcz022rXf2txJGYKmllaaWtrCZlgGrXWr1noKkAfMBCb09Im11k9qrWdorWdkZ3teHCCEEOHiu8cP99ru3nPvrMZMjTnHPS1ckruD1roKWAHMBjKUUo6BqDxgv7m9HxgGYO5PByoCEawQQoTCrxZN9tre1eTuuIApLHruSqlspVSGuZ0InA5sw0jyF5qHLQbeMrffNm9j7v9I+3OWQQghwsywzETOn5rrcym9OPcxdz+Te0YQkrs/ky1zgOeUUtEYHwavaq3fVUptBV5WSv0a+Bp42jz+aeAFpVQBcAS4pBfiFkKIXtfY3EpinPcLmABiY1yTvs3e8apM/1q9FwhOz73T5K613gRM9dJeiDH+7t7eBHwnINEJIUSI2FvbzGmLvhOxx7BMJ6syvfG1MXodFsMyQgjRH5VUNtLSqskfmOzzGPfkXtvkfdUmd5LchRAiRFbuLgdgdLbv5O4+5l7Z0OzXY4fdbBkhhOgv3v/mIABjsn2X5XWf517lZ3L3dYI2kCS5CyGEF21tkDcgscOrSd2TdJ3N9wnV1rbgThqU5C6EEF40trQysoPxdgD3yiptXmZ9X/S3VeTf8R4V9TYA7jr7qMAF2QFJ7kII4UV9B9UgrY7JS3duv7PxgMf+NXuOAPDPVcUAJMQGJ+1KchdCCDdaaw7XNPms4+7L9kO1Pve1mMMyCbG+580HkiR3IYRws7usnpomO2MHpXR6rL+nRl/80ui5d3RRVCBJchdCCDeOWS9H5aR1frDbuLv7idNR5lTKGnMOfKL03IUQIvi01vzT7GX7Mx/dfVajR30Zt3OsUUFa3kKSuxBCWHy6q5w3NxgnRtMSOh9zd0/V7sndfQEPfxb0CARJ7kIIYVFcUQ8YPfLcAYmdHu8+HXJ3eZ3LbXura9fd/cKn3iLJXQghLKoajLK8O359FvExnY+Pu6fqCx7/wuV2i6WnHhcTxfxxg3ocoz8kuQshhEV1YwvJcdEeRcF86WwI3ZrcL5yeR1QQSg+AJHchhHBR1dBCRlKc38erTiZD2i2zZxwXNAWDJHchhLCobmwOaNVGa8/9txccHbDH7YwkdyGEsOhsgQ53Pz59nM/58K1tmhbLCdVhA5J6HJ+/JLkLIYSpqLyejSXVZHRQCdLd7NFZLL11ntd9dTbXxTuCdQET+LeGqhBC9Avz//AxANNHDAjI47kn9/ggFQ0D6bkLIYSH8UN8L9DRFXVuy+7FxwQv5UrPXQjRr205UE1rm+ba59c520ZkdlzH3V/Vjcac+fysJIoqGjwueOpNktyFEP1WU0srCx/93KM9z48rU/2x90gDAM9cfhyjsjuvMBlIMiwjhOi3th6s8Wi7+oSR3brQaGCK59z4g1WNAOQFcZaMgyR3IUS/5V4qAGBybrqXIzv3t+/P8Girb24lLjqKuCCOtTtIchdCCIu0xO6NVrsvlg3wvy2HglYF0p0kdyFEv1RYVue1PS2he1enehvJKSyv79ZjBYIkdyFEv7TDXO/06cUzuOOsCc72wWkJ3Xq8YC3C4S9J7kKIfunLwgriY6KYPTqL608azaDUeABy0ruX3K3DMm1tGnuIhmMcZCqkEKLfaWpp5cXVe5k2YgBJcUYafO36OWw9WE2Mn6V+3Vl77hc+8QXr91YBMGtUZo/j7Q5J7kKIfuc/X+/H3qbJSm6fvjg8K4nhWd2fsmgdc3ckdoC5owd2+zF7QoZlhBD9jmO1JetYe0/5GnKPDtKyeu4kuQsh+p16m50oBcMze//iougQnWiV5C6E6HdqmlpITYgNSq0Xb/Pfg6HT5K6UGqaUWqGU2qqU2qKUutVsv1cptV8ptcH8d7blPncqpQqUUjuUUmf25gsQQoiuqm2yd/tiJd+8J/FQJXd/Xp0duF1rvV4plQqsU0otN/c9rLX+g/VgpdRE4BJgEjAU+EApNU5r3RrIwIUQortqGltIjQ/cUnodiQnXnrvW+qDWer25XQtsA3I7uMsi4GWttU1rvQcoAGYGIlghhOip2qYWth+q7YWeu3fdKUIWkOftysFKqXxgKrDabLpZKbVJKfWMUsqxdEkusM9ytxK8fBgopa5VSq1VSq0tKyvreuRCCNENi/68kv1Vjd0uM9BVYdtzd1BKpQCvAz/SWtcAfwVGA1OAg8Afu/LEWusntdYztNYzsrOzu3JXIYToNke9l9QAJ/dhmd5rwIeqLIFfyV0pFYuR2F/UWr8BoLU+rLVu1Vq3AU/RPvSyHxhmuXue2SaEEGEjOT6wi1XHx3h/vJhwneeujLlCTwPbtNYPWdpzLIedD2w2t98GLlFKxSulRgJjgTWBC1kIIXouWLNYtA7K03jw54zCXOD7wDdKqQ1m28+BS5VSUwANFAHXAWittyilXgW2Ysy0uUlmygghwkFNU4tzO1gXF5XV2oLyPO46Te5a68/xPoHz/Q7u8wDwQA/iEkKIgFu5q9y5PXt0VlCes95mD8rzuJMrVIUQ/UaxuWD1V3edxqlHDQ74498ZwFo1PSXJXQjRbyxZuh3ApRpkIF130mjndmKscYL1+vmjfR3eq6TkrxCi3wnGhUV/vWwa88cP6vXn8UWSuxCi30iNj+HCGXlBea5Q1ZRxkOQuhIh4jc2tPPVZIbU2OxmJvTMk4xCloE2HrtSvM46QPrsQQgTBmqIjPLR8JwAjs5N79bkcZYRD3XOX5C6EiHgNlumI88b07rJ3jpQuyV0IETRaa1pa20IdRtDVNxvXUf5q0SQG9NJMGQdHLZlQVYN0xhHSZxdCBNWSZdsZe9dS7P0swTc0Gz33s47O6eTIADBzuoy5CyGC5m+fFAJQ3djSyZGRpd5m9NyT43p/Domjwy7DMkKIoKts6F/JvaaphZgoRUJs76c8hZxQFUL0ot8t285Zj3zmdV9VQ3OQowmtwzVNDE5LCMqC2HExRlqV5C6E6BWPf7ybbQdr+HxXOYdrmlxOpN7+740hjCy4Vmwv5Y31+53j7r0tKc4oOxDq5C4XMQkRgbSliPhlT68mOkqx7NZ5zrbiiga01kHpyYbaa+tLgOANRTmSe2tbiAq5m6TnLkQEqnUrM9vapllZUO7S1hzBM2YcH24llQ28t+lgUJ87Jd7oMzc0h3YZC0nuQkSgQ9VNHm1ldTaXoYJmexvbDtbQbI+sJP/Up4WMu3spXxZW8PjHu53tL187KyjP/7sLj+X0iYOZmJMWlOfzRelQrQFlMWPGDL127dpQhyFExPhkZxmLn/G+uuUN80fzV0vSOzYvnZomO0tvnUdCbGDXFQ2F8XcvxWZ+YB2Tl86mkmoAipYsDGVYvUIptU5rPcPbPum5CxGBDlU3+tw3PDPJ5fbGkmr2lNezz1zIoq9Ljm8/lehI7P2RJHchItDB6iZ8nSuNi/b+Zx8f0/d77eD79fU38lMQIgIVldeTk5bg0f7uLSc452G7s7dFxti7t9c3YUhqCCIJLUnuQkSQXYdryb/jPd7ccIAxg1P5/YXHkBzX3iMfmBLfQXIP/fm3QBiQFOty+66zj+Lf188OUTShI/PchYggn+1qn+44dlAK35kxjAum5bGnvJ7S2iaGpCew/ZD35B4p1SJrbXYWHp3DDfNHU1RRzznHDA11SCEhyV2ICJJo6aWfMsFYvzM6SjFmUApjBqUAMCTdc7gGoKW17/fctdYcqm5i/rhBTM5NZ3JueqhDChkZlhEigtSY1R5/tmA8c0ZneT0mP8tYiejOsyZwzbyRzva+VAa4sbmVI/We9XE276+hobmV3AGJIYgqvEjPXYgIcqShmbiYKG44abTP0gIJsdHs+e3ZKKXYVFLFU5/tAYLfc9daU2ezk5oQ2/nBbi584gu2HKihaMlCtNbc8M/1HD8qk73mdM7547MDHW6fIz13ISJIZX0zmUlxndaMcewfnZ3ibAv2bJlnVhZx9L3/40CV7zn5vmw5UOPcrmm0s2zLIe57ZysVdc3kpCe4vK7+SpK7EBGkpLKxS8vIJcfH8J8b5wDw0PKdtAVxxsyv3t0KGDF3l83eyjubDjhvVzY0M9jLFND+SJK7EBHg813l5N/xHl/sruCEMd7H2n2JNS/6+XpvFZsPBP+Kzjqb/9UaW9s01z7fXqrkkx1l3P3mZuftz3aVB2VBjr5AfgpCRIArnm2vI3PqUYO7dN+Y6PYhnJ70ov1V3dDCIx/sct6uqPN/4ZA95XX8b+th5+1rX1jncUxReWSUUegpSe5CRADrydBpwwd06b7Wy/WrglDzfPm2wzz8wU7n7Xqb/4toFJbVd3pMpFxp21OS3IWIAPExUZx77FB2/+Zsn1eg+pJiKbRV1dj7y+8dqbe53K73s+75xn1Vzp767aeP83ncsXkZ3Y4tkshUSCH6OHtrGzZ7G6OzU7q1tJu1imJ1EHrue8pde991nfTc9x1p4Ly/rKTCMq99zpgs/rjc+/GPXDq1xzFGgk4/4pVSw5RSK5RSW5VSW5RSt5rtmUqp5UqpXeb/A8x2pZR6VClVoJTapJSa1tsvQoj+zNHzTY7vXlXHREsN96aW3l09qKW1jXc3uq6M9O+1JR3eZ9XuCpfEDvicEfPytbNcvon0Z/58f7MDt2utJwKzgJuUUhOBO4APtdZjgQ/N2wBnAWPNf9cCfw141EIIJ8eYdXeTWpR1daZevkr1SH0ztTY7Z05qP+nb2QfKpv1VLreT4qK9LioyKDWemfmZAYkzEnSa3LXWB7XW683tWmAbkAssAp4zD3sOOM/cXgQ8rw1fAhlKqZxABy6EMDSaydFaV6arrjrBKENga+m95P7JzjKO/82HAOQPTHa219ns1DR5Hw5qa9Os2XPEefs/N85h6/0LPM4rvH7DbNbcdZrLB1V/16WPeqVUPjAVWA0M1lo7vl8dAhwfxbnAPsvdSsw2l+9iSqlrMXr2DB8+vKtxCyFMjjVQe7JIxS/OmciKHaXYerHnbl3276Sx2QxOTUBjXMxUVmsjza0Mwfq9lVzw+BfO26kJMUw1ZwJZX+t1J45i+gjpsbvz+7dBKZUCvA78SGtdY92njYVYu3Rpm9b6Sa31DK31jOxsqQMhRHc5k3sXZ8m4i4uOCspi2bkZiUwcmsaVJ4xkrFmp0lsRMGtiB1h39+nObWty/+mZ43sp0r7Nr567UioWI7G/qLV+w2w+rJTK0VofNIddSs32/cAwy93zzDYhRC9w1GGP7eHycvExvZfctdYkx0Vz/rRc7v3WJGLMWDPNUgnekrvVI5dMcfnwsg6/xMiyel75M1tGAU8D27TWD1l2vQ0sNrcXA29Z2n9gzpqZBVRbhm+EEAEWsJ57TBQ2e2Bny9TZ7JTX2dhVWkd9cysThqS5JOMk8zyB+0lVrTXW4fNFU3IDGld/4E/PfS7wfeAbpdQGs+3nwBLgVaXUVUAxcJG5733gbKAAaACuCGTAQvQnlfXNlNfZGDvY+xqgqwsr+O7fVwM977nHxUTR6OcFRf4659HPKKpo4PqTRhOl2hcQcXDEbLN8Y2hotjPlvuW0aRiRlcSCSUMCGlN/0Wly11p/Dvg6BX2ql+M1cFMP4xJCAFc/v5Z1xZV8/YvTvVZ7fGVt+9yF+B723FPjY1lZUMFtr27goYum9OixHIoqjDovWw/WcFROGkMzXBfRcMRsXeJvT3m9c0rmjfNHc/FxMuGiO2SwSogwpbVmXXElAKc//KnH/oLSOt5Y3346q6fDMo7pidbHDJTCsjpyMzxXR3L03K1j/fstxcumDPNdJ+e+cyfxz6uOD2CUkUWSuxBhylqhsbzO5rxsv7VNc+WzX3HaQ5+4HN/TYZns1Pge3b8jJZWNXq8qdXwg3ffOVucyf47XvfTWeYwf4n04CmDxnHxOGDuwF6KNDJLchQgjR9/zX37z/jYAdhyqBWCemcA+3GaUuv1sVxkfbS913sdRvzw1oWeX3cf08gVAk4amebRZP5BWmxcrHahqJCE2igkdJHbROSnCIESYaGpppdZm58lPC0mIieLRjwoAePDbx3DWI5/x2roSrp43iv2WZem23HcmBaV1FJbXMTClZz3v1gCvwtTidkHUjHzPIZZYSy35VbsrmDtmIFWNLQzwY6lA0TFJ7kKEiYPVTc5tR2IHyEqJY9rwDNYWV9LU0spd/2lfeSg5PoZjh2Vw7LCMHj9/mw5scn/0w10ut72ta2pN4CWVxsnX2qaWHn8LETIsI0TY+Ga/5xJ3qfExxMdEk5YYS22TnXP//Llz34Zfnu5xfE9M7eIiH53ZWNL+ek6dMKjTnvibGw5gs7dS22T3KEUguk4+HoUIE6sLK0iOi+aaE0eRlRLPZccPd66wNH98Nm9tOMDOw3XO4zOS/F8I2x/TRwzgirn5/GNlEUfqm/nLigL+b8GEbs/CSTFLEK//xel+98SP1DdT09RCdg+HmIT03IUIG2v2HOG4kZn86LRxfH/WCJRSzsR6/tQ8RloqKSb3oAJkRxzj9ve9s4WnP9/D0s3dv7i8uKKB+eOzyUyO63AmT9GShc7tirpmymptPT5/ICS5CxEU/pysPFTdRH5Wss/9jrVBL505jPd+OC9gsVk56qRX9nBFJq01eysaGJGZ1KX7ldXaKKu1MSTd+2Icwn+S3IXoZeV1Nkb//H1e+Wpvh8c1trQ6a614c8spYwG4e+FEl3rogeSYVvnpzjKAbs9YOVTTRK3NzogOPqys/nPjHAB2HK6lTcMgHystCf9JcheilzkuPnry00KfxzTb27C36Q6T+0UzhlG0ZKHLmqeBlhDj+vzR3UzuXxRUADB7dJZfx48aaMyk2XrAqCY+RJJ7j0lyF6IHmu1tnPHwJ/zizc0e+z7dWcbPXtvIAXNe+u6yetrchmfKam1orZ0Fu7wtHxdM7s/f2s3pkXvK64mOUowZ5Dn90Zu0xBhS4mN4e+MBoHevlu0vJLkL0QOF5XXsPFzHC18We4yrP7hsO6+uLeExy5z1T3eVObfrbHaOe+ADbnt1Iw0txjqoSXGhncDmGJZx6O6C2eV1tk5PpFoppZhj6eXLItc9J8ldCDf21jae+6LI67qeH+8o5d+WSozWpeNeX1/icqyjUFZBafv0Rev2viPGRTv/+Xo/e8qMoZuUEF+8k5HkOr+8O8m9zmanqqGlywn6ppPHOLeT40P7DSYSyMejEG4+Lyjnnre38GVhBdecOIqKumZOn2gsEXz5P74CIG9AEgmxURyusTnvd7CqyeVxKtxWF0qIjWL51sNcPW8UAMUVRkKPi46ioMxI+tOGZ/TKa/LXNLcLmQ64vabOVDe0cOz9/wNgYo5nLZmOWK+yTYqV1NRT0nMXws3HO4yhkzV7jnDFP77imufXUtXgmqhv/td6zjfX+Pzu8cPJTI6jtNY1EZbXtSf+K+bmk5Oe6OytA/xvq1EIbHhWknPMPdNLzfZgcp8ds6+ywceRhoZmO5c++SVvbdiP1tqZ2MGYMdNdib00j78/kY9HIdysMasTWnven+wsIyaqvS9UZ7M7t68+YSTriysprW1P5j97bSPFFQ1cMTefe741CYDY6G08+WkhyzYf4oNth5110+2tbTSawx/us1VCafzgVOqa7B0eU1hWz6rCCgrL65g+wrXX39m6qN7kZiSyv6qxx7XphSR3ITzsPdLA4LR4lyGXW1/e4NzOTo2nzEzkT1w2jVHZKWgNy7ceRmuNvU3z6lpj/N1awzzL7JVf/891Ls9XZ2ulsaWVuJgol4WfQy0zOY46m522Nu0zrkrzG82R+mbOecyoe3PKhEGs2FHKDSeN7vJzvnXzXOfUUdEz8vEohEVLaxt1NjuTh6b7POaiGXnO7TPN9T0dJ18Ly+upt/TqrePO3oZcLpiaS0OznabmVhJDPA3SYd3dp/H1L04nOT6adcWVjPr5+z6PdVzJ2tKqqTK3H7t0Kjt+dRY/WzChy889MCWe4/Izuxe4cCHJXQiLr8whmYleFpYAY2m3hUcPdd52jFH/8pyJgNGDdQzZXD4n37nQBuC1XkrugEQamlupD6PknpUSz4DkOLYdrO302Eq3oZcnLptOcnyMDKuEARmWEcLiu39fDcDIgcmMHJjMhdPzGJ2dzN8+LaSmsYUTx2UzcmAyvzxnorPWC8Aws4bKC6uKufFkYzjiuPxMlxOUWSmePXfHdMHSWlvYTf+zLgqitfZaisAxLHPl3JHcePJoKfgVRiS5C+FFTnoiK34y33l7weQclwR35QkjXY4fNsBI7m9vPMDZRxtDNe5z1t2HZV67fjbbzKX09h1pCLurMlfecQrnPvY5FfXN2OxtXq+eLSyrJy0hhl9+a2IIIhQdke9OQnjhPvMDOi6ilZ4Uy7jBxqX2u80LkrLcknlWcnvyHpwWz4z8TGfN8z3l9Qz00rMPpdyMRH54qlGs7Cf/3uixX2vNJzvLGBhmH0rCIMld9AsVdTZnUSpftNZEKbjllDHdGjP+P/MEYqGZ3AeluSY9x9zt7NR4Vv/8NAAm5rSfuA31HHdv0hKNbx/vbvKs636oponqxha+P2tEsMMSfpBhGRHxGptbmf7rDwDY89uzffbA65tbadN0e4k3R7XG19eXEBcT5dJTd3jrprkMNcsSAM7ePuCcXhlOJnUwa8hx4dWAAK8IJQJDkruIWKU1TWjgy8IKZ9vKggpOsMxgsappNKbyOXqrXWWtpTJqYDLRXuaGuy9kbf2gCZfZMlbjBqcyIivJawJvbjVOKMvMmPAkyV1EpCc+2c2SpdsB4wpSh8ueXs0bN87xqKGyZs8RZ5Lqac8dutabLfzN2Tz1WSEXTs/r/OAQmJiTxi5LwTOHZruZ3P2s/CiCS94VEXG01s7EDvD3z/e4VDu84PEvXKodflNSzUV/W8VPzZOGaYndTe7tPe+OFt1wFxWluO6k0WSF6TTC1IQYar1UyHQmd+m5hyV5V0TE2XvEs9jVD2bnu9x+7KNdzu0tB6oBnL3T7vbcrcMybd1c5CIcpSXEUtNop6apherG9iRvk+Qe1uRdERHn0Q8LPNp+fNpYXrpmFslmj7qovP0DoKSy0eXY1G7WVLeOmTsSXyRITYilsaWVY+79Hzf/az1gXOD0PfOCL0nu4UneFRG2vimp5pdvbXbOyqhuaPFr8QjrohmTc9P46ZnjUUoxe3QWW+5fwLyxA52lbDfvr+bPK1w/DHIyurd+p/XkaHMEJXfrCebPdpWjtWbuko+cbTLmHp7kXRFh619rinl+VTHPrSriSH0zx97/Pyb8YhmbSqr8foznrpjpssIPGKUCiisaKCqvd1YydPQ+7154FPE9KLt7w3yj9IC/a4f2BamWYarM5Dh2HHatORNBI1ARRWbLiLC0v6qRl9YYy9ktWbrd5QTp+uJKjsnL8LhPW5vmFXMJvG9Py+P8qbleT1LmDUikurGF+X/4GIDvTM/j9985NiBx3376ONITYyPqwp40yzBVnc3Ogj995rJ/uFlXR4SXTnvuSqlnlFKlSqnNlrZ7lVL7lVIbzH9nW/bdqZQqUErtUEqd2VuBi8i29BvPKyId6pu9D828uWE/d77xDQBH5aT6nM/uqAPj4LjEPhBioqO4/qTRLtMi+zprz919uOmsyUNIT+reCWjRu/z5DXwW+DPwvFv7w1rrP1gblFITgUuAScBQ4AOl1DitdfeWUBf9lnVWBsCbN81lx6EafvnWFufFRlYNzXZue7W9/om3IlcOwyw9zaIlCwMQbWTzdVHXj04by8XHDQtyNMJfnfbctdafAkf8fLxFwMtaa5vWeg9QAMzsQXyin6pqaGGApUc4ZVgGFx83nNSEWGq8LP3mPkOmo3nmE4akAnB0ru9L60U7X1NDf3TaOHLSE73uE6HXk++ONyulfgCsBW7XWlcCucCXlmNKzDYPSqlrgWsBhg8f3oMwRCSqbGgmIymO315wDNB+xi4tIca56pFDS2sbr60rcWnrqK54Qmw0r98wm+GZyQGNOVJ5mxr6kzPGhSAS0RXdnS3zV2A0MAU4CPyxqw+gtX5Saz1Daz0jOzu7m2GISFRa08S7mw6SmRzHgslDWDA5x7kvNTGWWreee0FpHeV1Nv508RT+dfXxTM5N81qy12r6iMywq58erlLczh+cNC6bm08J3HkK0Tu61XPXWh92bCulngLeNW/uB6yDcHlmmxB+e3DZDsBzCTeAw9VNbNxXxT1vbWZSbjqLpgxlh7ngxVE5aYwfksq7t8wLaryRLsZtHvtZk4eEKBLRFd3quSulciw3zwccM2neBi5RSsUrpUYCY4E1PQtRRLp9RxpcZmGU1Rmlb39xjufqPodqmgB4blUxP3ttE+PvXuYssTsqW4ZZess/Lj/Oue1e2VKEJ3+mQr4ErALGK6VKlFJXAb9TSn2jlNoEnAz8GEBrvQV4FdgKLANukpkywhd7axu3vbKBeb9bwQ9f+hqAe9/ewqc7yzhxXDYnTxjkcZ8/eJmP/tmucq47cRSxcqVkrzl5wiCGpBlX7roP04jw1Om7pLW+1Evz0x0c/wDwQE+CEv3D+r1VvPG1MWq3bMshPtp+mGe/KALgKHNGi7sLp+d5XfJt4TE5Xo4WgeSo397RNFMRPqSrI0Ji64Earnz2KwBevnYWAFc+uxaAy2YN5/Yzxnf6GLseOIvFs0cwYUgq4wZ5/zAQgTNpaBrQtXLGInTk+5UImrY2TXNrG4eqmzj70fZL2CcOTWNiThpbDxprnN69cGKHlQbX3n0ajc2txEZHcd+iyb0etzD85XvT2HW4LqKuvo1k8i6JoBn18/c92q47aRRpCbG8c8sJPL6igLTE2E6/9nc0h130nrSE2E6nmIrwIcldhEROegIPXzyFWaOyAIiOUtwSwBovQvR3ktxFr6ttauGyp40ZsTefPIaFx+QwYUiqS/1zIURgSXIXve62VzeycV8VAIumDGXsYDn5KURvk+QuetWmkiqWbz3MkLQEPv+/kz2udhRC9A75SxO9alOJsfj0K9fNksQuRBD16b+24op6rn7uK6obPOt7OxyuaeJ3y7Zjb42cNS37ijqbnbvfNCpTyGo9QgRXn07uhWX1fLCtlPve2eLzmCVLt/P4x7v5dFdZECMTAC9+WQzAVSeMlJOnQgRZn07ujtojb3y9n+VbD3s9pqnFKG1TVmsLWlzC8GVhBWMGpXgtACaE6F19OrkDvH7DbACueX4t97+zlZLKBjaVVDn3O6oI1tmkflkwrdpdwYodZRyXLxe9CBEKfT65Tx+RyU/PNOqQPLNyDyc8uIJz/7ySPeX1aK0pOFwHQL3Nc2k20Xs2mFMfvztzRGgDEaKf6vPJHeDyOfnkpCe4tJ38h4+59eUN1JpJvU6Se1A56rNPNItNCSGCKyKSe3J8DKvuPJVt9y/g6cUznO1vbzzg3F66+WAoQuu3GltaiYuJIjpKTqQKEQoRkdwdEuOiOfWowYwdlOLS/oPZI9h3pNF5laTofU0trSRK3W8hQiaikrvD+7fO46GLjBV7cjMSWXi0sZDDor+sRGvdrcesaWrh9lc3cqCqMWBxRqpmextbDlSTEBuRv15C9AkR+dcXGx1F3oD2i2amjxjgHJPfcbi2W4/5yY4yXl9fwpwlHwUkxkh2/7tb+KqoksM1Mv1UiFCJyOQOkJkcB4BSxurtz185E4CPd5Sxv4u9b601t5hrfAJsOVAduEAjyPZDNdz+6kb++eVeAH582rgQRyRE/xWxhcNGZCVx0rhsbjllDNC+wMOSpdtZsnQ7RUsW+v1Y176wzuX2+r1VLHz0c44dlsHsUVlMGJLKeVNzAxd8GLC3ttGm6XBFJKvSmiYW/Kl9daV7vzWRy+eO7K3whBCdiNjkHhsdxXNmbx0gPTGW6ChFa5sx5l5a28Sg1ARfd3fhuPr1h6eM4dGPCnh9XQkAG/dVOU/SnjxhEOmJsQF8BaGxsqCcf63ey/Kth0lLjGXt3ad1eHydzU5BaR1ri44AcExeOs9fOZOMpLhghCuE8CFih2XcRUUp51ANwMwHPnQpOLZ5fzXPrtzjcb/G5vYrW+eMGQi0X6Bjta74SACjDQ17axvf+/tq3vvmIM2tbZTX2To8Ab2yoJzJ9/yX8/6ykl+/t40RWUm8ffMJktiFCAP9JrmDZ32ZNUXtCfmcxz7n3ne28n+vbXI5rqK+fXtQajzHj8wEYFhmIvd8q71myo9f2dhbYfeaNXuOcNETq5wfcit2GMXVTjtqkPOY6kbvFTf3VzWy+Jk1Lm0LJg3ppUiFEF3Vr5K7+wU1Ow7VAFBZ3+xse2XtPi596kvn7S8L2z8AMpLinMXK9h1pZNGUXEZnJwMwxm1ufV/w/Koi1hQd4dj7/8dN/1rPZ2blzEcumcqr1xk1ez7Z6b2a5o0vrsfepnnhqpl8c+8ZPHzxsfzELAMhhAi9fpXcP/7JfGcyTomP4dkvipm75CNeMEvTOhSU1jm3l289BMBvLziazOQ4bC3tdeEzk+P48Pb5nDQum4bmvleYzHGSGeC9TQd5flUxU4dnkBwfw/QRA8hKjuPZL4o87tfWpp3nGuaNzSY1IZbzp+YRK4txCBE2+tVf47DMJN68aS5Lb53HiKwkyuts7K9q5KHlOwF44PzJjByYTHSU4uU1e8m/4z3+u+Uwk4amcenM4QDkDzTmzz926VTn404fMYBtB2soLKvzfNIuqqxv5h8r93DcAx9QXte788RrvAy5/Ob8owHjW845x+Tw9d4qymqNsffFz6wh/473+HB7KQALj8np1fiEEN0XsbNlfElNiOWonFiS4zxf+qXHDafF3sa972zljje+cbZbh1zOPXYoYwalMGlourPNUe7g3D+vZPN9Z3Y7tg+2Hubq59c6b+84VMvAMfEd3KNnKhuaOTo3nXduOYFPd5YxKjvZ5eKvo/MygGKOe+ADLp+T7xyiucaM8ab5Y3otNiFEz/SrnrtVYpxn3ZOoKIW9zXN2yIPfPsa5rZRySewAc0Ybs2iGZiRQUWfr1pJ+WmuXxA6w70hDlx/Hm6qGZn63bDuL/rKSxz7c5Ww/0tBCRpIxffPEcdkuiR3gAsvcfcfwzLjB7R90R+WkBiQ+IUTg9bueu0OSW3J/95YTAJg9OguA+eOzmTc2myvn5ne6RFx6Uizzxg7ks13lTP/1B3x/1gh+dd7kLsVTXtfs0VZS2fM6Nlpr5iz5yHlOYOO+KibkpPHUZ4Vs3FfFoilDfd43KkoxMCXOGdtROWn8ffEM5i75SJbOEyLMSc8dmDY8g8m5Rm980tB0ipYs5NkrZnYpgdns7b119xO0nWlpbWPJ0u0A3L9oEo9cMoXM5Dj+vKKgx4uM7DvS6EzsjvH0a55fy5o9xiygrOSOh31W/GS+c/vYvHRyMxIpWrJQls4TIsz12+Tu6LlfOnMYr5jT/nrEbTTH3+qTVQ3NzP7tR7y+3rjqdc7oLBZNyeVb5snK9zZ1vw59Wa2NE3+/AoBlP5rHd48f7vGNYmhGx1fppibEOle68jaUJYQIT/02uacmGGPNA1PiAzKF7+FLpvDbC47m0pnDANhU4l9xsXXFlS6zYhzj3r84ZyIThqTyxCe7afNyHsAfT36627k9bpAxPv69mcP56ZnjnXP+8wYkdvo435mRx7yxA7lh/uhuxSGECL5+m9xjzeTWbO/6yU9vcjMSuXTmcC45zpgy6U/lyU92lvHNfuNDYN3dp/HR7SeRYC5wERMdxaUzh1NYXu9c5LurHBdgLbngaKLM1xsVpbjp5DHOk6HZqZ3PxhmUmsALVx3vdy0eIUTodZrclVLPKKVKlVKbLW2ZSqnlSqld5v8DzHallHpUKVWglNqklJrWm8H3xBmThpAaH8O3p+cF9HFzzZ5wqZmQfQ3P7Dxcy+Jn1vCnD3YxJC2BrJR4RmW7XuU6wKyF09DcvXH3QzVNfHtaHpeYc/Stbj3VKMc7ZpDMeBEiEvnTc38WWODWdgfwodZ6LPCheRvgLGCs+e9a4K+BCTPwJuem8819ZzJucGCTW2ZSHDFRitJaG6U1TYy8833e/Hq/x3GrCyuc2+dP814uOMnsxbtf/VpcUU9TS+dXxNY12clM9l6p8vSJgylasjAiKlkKITx1mty11p8C7iUPFwHPmdvPAedZ2p/Xhi+BDKVUv7qM0Zg+GM/jH+9m1m8/BOBHr2zwOM6xoMV5U4ZyzbxRXh/LcdK33taeyMvrbJz6x0+46G+reHF1MeuKKz3u98nOMs565DMaW1pJ8nKxlhAi8nX3L3+w1toxjeMQMNjczgX2WY4rMds8pnwopa7F6N0zfLjnsEFflpUSx6GaJrydB91xqJbhmUkcqmniB7NHcP8i3/PhHbNTGlvah2XWF1dib9NsKql2nrS1LjzS1NLqUq0xOV5muAjRH/X4hKo2BpW7PJ1Da/2k1nqG1npGdnZ2T8MIK97mpq8urKDeZufMP33KSb9fQXVjCxmdDIkkxxufvXWWnvvXXmrJ3/bqBuf2P1YWueyLkguNhOiXuttzP6yUytFaHzSHXUrN9v3AMMtxeWZbv5KVEk9RhWvpgIuf/JLvmCdvS8168UPSO56G6FhcpMIyVXJ9cSXH5KVz+Zx8kuJiuP6f63hj/X7iY6KZNjyDB5cZF0Pddvo4Hlq+0+9l8oQQkaW7yf1tYDGwxPz/LUv7zUqpl4HjgWrL8E2/seSCo9lYUk18TJTLwtr/Npfnc/j29I7XXc00VzS6752tZmndGFbvOcJVJ4zkgmnGB4Vj6b+X1uzlpTXGOP7Ti2dwyoRBzMgfwPQRAwL50oQQfUSnyV0p9RIwHxiolCoB7sFI6q8qpa4CioGLzMPfB84GCoAG4IpeiDnsjR2cylhzFs6KHaW8sd7zy8uvz5tMfEzH4+FRlsVFzn7kM5rNgmRzx2Q52y+bNYJHPypw3r7upFGcepRxCsRR0EwI0f90mty11pf62HWql2M1cFNPg4okp0wYxBvr9/PTM8ezbPMhZo7M7FJdll+fN5m739zsTOwAJ41rXwZvUFoCb900l0V/WQnAnWcdFbjghRB9lsyT62XnHDOUMycNITY6iptO7nr988tmjWDKsAzOeexzAB6++FiP5QLHDzG+JZx7rO8Kj0KI/kWSexD0tHbN5Nx0FkwawrIth1yWxnNIiI1m1Z2nMMAcoxdCCEnufcRtZ4xjVHYys0Zled2f08nMGyFE/yLJvY8YNziVny2YEOowhBB9hEyCFkKICCTJXQghIpAkdyGEiECS3IUQIgJJchdCiAgkyV0IISKQJHchhIhAktyFECICKV8LOAc1CKXKMKpLdsdAoDyA4YRCX38NfT1+6Puvoa/HD33/NYQi/hFaa6+rHYVFcu8JpdRarfWMUMfRE339NfT1+KHvv4a+Hj/0/dcQbvHLsIwQQkQgSe5CCBGBIiG5PxnqAAKgr7+Gvh4/9P3X0Nfjh77/GsIq/j4/5i6EEMJTJPTchRBCuJHkLoQQEahPJ3el1AKl1A6lVIFS6o5Qx+ONUmqYUmqFUmqrUmqLUupWsz1TKbVcKbXL/H+A2a6UUo+ar2mTUmpaaF+BQSkVrZT6Win1rnl7pFJqtRnnK0qpOLM93rxdYO7PD2ngJqVUhlLqNaXUdqXUNqXU7L70Hiilfmz+/mxWSr2klEoI9/dAKfWMUqpUKbXZ0tbln7lSarF5/C6l1OIQx/9783dok1LqP0qpDMu+O834dyilzrS0hyZPaa375D8gGtgNjALigI3AxFDH5SXOHGCauZ0K7AQmAr8D7jDb7wAeNLfPBpYCCpgFrA71azDjug34F/CueftV4BJz+wngBnP7RuAJc/sS4JVQx27G8hxwtbkdB2T0lfcAyAX2AImWn/3l4f4eACcC04DNlrYu/cyBTKDQ/H+AuT0ghPGfAcSY2w9a4p9o5qB4YKSZm6JDmadC9gsbgB/8bOC/ltt3AneGOi4/4n4LOB3YAeSYbTnADnP7b8ClluOdx4Uw5jzgQ+AU4F3zD7Dc8kvufC+A/wKzze0Y8zgV4vjTzeSo3Nr7xHtgJvd9ZoKLMd+DM/vCewDkuyXHLv3MgUuBv1naXY4Ldvxu+84HXjS3XfKP4z0IZZ7qy8Myjl94hxKzLWyZX4+nAquBwVrrg+auQ8BgczscX9efgJ8BbebtLKBKa203b1tjdMZv7q82jw+lkUAZ8A9zaOnvSqlk+sh7oLXeD/wB2AscxPiZrqNvvQcOXf2Zh9V74eZKjG8bEIbx9+Xk3qcopVKA14Efaa1rrPu08ZEelnNSlVLnAKVa63WhjqUHYjC+Xv9Vaz0VqMcYEnAK8/dgALAI40NqKJAMLAhpUAEQzj/zziil7gLswIuhjsWXvpzc9wPDLLfzzLawo5SKxUjsL2qt3zCbDyulcsz9OUCp2R5ur2sucK5Sqgh4GWNo5hEgQykVYx5jjdEZv7k/HagIZsBelAAlWuvV5u3XMJJ9X3kPTgP2aK3LtNYtwBsY70tfeg8cuvozD7f3AqXU5cA5wPfMDygIw/j7cnL/ChhrzhiIwzhx9HaIY/KglFLA08A2rfVDll1vA44z/4sxxuId7T8wZw/MAqotX2ODTmt9p9Y6T2udj/Ez/khr/T1gBXCheZh7/I7XdaF5fEh7Z1rrQ8A+pdR4s+lUYCt95D3AGI6ZpZRKMn+fHPH3mffAoqs/8/8CZyilBpjfYM4w20JCKbUAY4jyXK11g2XX28Al5kylkcBYYA2hzFPBOjHRSyc7zsaYfbIbuCvU8fiI8QSMr56bgA3mv7MxxkA/BHYBHwCZ5vEK+Iv5mr4BZoT6NVhey3zaZ8uMwvjlLQD+DcSb7Qnm7QJz/6hQx23GNQVYa74Pb2LMvOgz7wFwH7Ad2Ay8gDErI6zfA+AljHMELRjfnq7qzs8cY2y7wPx3RYjjL8AYQ3f8LT9hOf4uM/4dwFmW9pDkKSk/IIQQEagvD8sIIYTwQZK7EEJEIEnuQggRgSS5CyFEBJLkLoQQEUiSuxBCRCBJ7kIIEYH+H+RUbQ0eojJUAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "71433ba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "b75bab04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       132.045\n",
       "1       131.780\n",
       "2       130.280\n",
       "3       130.535\n",
       "4       129.960\n",
       "         ...   \n",
       "1253    314.960\n",
       "1254    313.140\n",
       "1255    319.230\n",
       "1256    316.850\n",
       "1257    318.890\n",
       "Name: close, Length: 1258, dtype: float64"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "71984105",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler=MinMaxScaler(feature_range=(0,1))\n",
    "df1=scaler.fit_transform(np.array(df1).reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "cde804e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.17607447]\n",
      " [0.17495567]\n",
      " [0.16862282]\n",
      " ...\n",
      " [0.96635143]\n",
      " [0.9563033 ]\n",
      " [0.96491598]]\n"
     ]
    }
   ],
   "source": [
    "print(df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "b0bb9e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_size=int(len(df1)*0.65)\n",
    "test_size=len(df1)-training_size\n",
    "train_data,test_data=df1[0:training_size,:],df1[training_size:len(df1),:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "f6f4ea7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(817, 441)"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_size,test_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "736af7e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.17607447],\n",
       "       [0.17495567],\n",
       "       [0.16862282],\n",
       "       [0.1696994 ],\n",
       "       [0.16727181],\n",
       "       [0.16794731],\n",
       "       [0.16473866],\n",
       "       [0.16174111],\n",
       "       [0.1581525 ],\n",
       "       [0.15654817],\n",
       "       [0.16271215],\n",
       "       [0.1614878 ],\n",
       "       [0.1554927 ],\n",
       "       [0.15443722],\n",
       "       [0.15730811],\n",
       "       [0.15604154],\n",
       "       [0.15849025],\n",
       "       [0.15308621],\n",
       "       [0.15735033],\n",
       "       [0.15490163],\n",
       "       [0.15946129],\n",
       "       [0.15688592],\n",
       "       [0.1537195 ],\n",
       "       [0.14434687],\n",
       "       [0.14812547],\n",
       "       [0.15308621],\n",
       "       [0.15241071],\n",
       "       [0.15055307],\n",
       "       [0.14924428],\n",
       "       [0.13607194],\n",
       "       [0.12551718],\n",
       "       [0.13906949],\n",
       "       [0.14911762],\n",
       "       [0.14890653],\n",
       "       [0.15401503],\n",
       "       [0.16115005],\n",
       "       [0.16583636],\n",
       "       [0.17618002],\n",
       "       [0.17060711],\n",
       "       [0.14725998],\n",
       "       [0.14700667],\n",
       "       [0.14422021],\n",
       "       [0.13691632],\n",
       "       [0.13949168],\n",
       "       [0.13784514],\n",
       "       [0.13522756],\n",
       "       [0.13071012],\n",
       "       [0.11863548],\n",
       "       [0.10259225],\n",
       "       [0.1058009 ],\n",
       "       [0.10466098],\n",
       "       [0.10630752],\n",
       "       [0.12403952],\n",
       "       [0.09773706],\n",
       "       [0.10512539],\n",
       "       [0.10474542],\n",
       "       [0.10816516],\n",
       "       [0.11323144],\n",
       "       [0.11044499],\n",
       "       [0.10415435],\n",
       "       [0.09419066],\n",
       "       [0.06510175],\n",
       "       [0.05395592],\n",
       "       [0.0565735 ],\n",
       "       [0.08169383],\n",
       "       [0.09533058],\n",
       "       [0.09689268],\n",
       "       [0.09465507],\n",
       "       [0.07337668],\n",
       "       [0.09288187],\n",
       "       [0.08456472],\n",
       "       [0.07992063],\n",
       "       [0.09275521],\n",
       "       [0.0836359 ],\n",
       "       [0.09385291],\n",
       "       [0.10077683],\n",
       "       [0.10542092],\n",
       "       [0.10951617],\n",
       "       [0.11006502],\n",
       "       [0.09955248],\n",
       "       [0.09756818],\n",
       "       [0.10499873],\n",
       "       [0.09735709],\n",
       "       [0.10124124],\n",
       "       [0.10411213],\n",
       "       [0.10288778],\n",
       "       [0.09330406],\n",
       "       [0.07903403],\n",
       "       [0.08426919],\n",
       "       [0.08122942],\n",
       "       [0.08460694],\n",
       "       [0.0862957 ],\n",
       "       [0.08853331],\n",
       "       [0.0862957 ],\n",
       "       [0.08089167],\n",
       "       [0.09195305],\n",
       "       [0.08975766],\n",
       "       [0.09055982],\n",
       "       [0.08388922],\n",
       "       [0.09085536],\n",
       "       [0.0873934 ],\n",
       "       [0.09030651],\n",
       "       [0.09891919],\n",
       "       [0.09887697],\n",
       "       [0.10622309],\n",
       "       [0.1213375 ],\n",
       "       [0.10529427],\n",
       "       [0.10221228],\n",
       "       [0.12213966],\n",
       "       [0.12745926],\n",
       "       [0.1231107 ],\n",
       "       [0.1302035 ],\n",
       "       [0.13607194],\n",
       "       [0.13366546],\n",
       "       [0.1291058 ],\n",
       "       [0.12969687],\n",
       "       [0.12762813],\n",
       "       [0.1115849 ],\n",
       "       [0.10879845],\n",
       "       [0.1071519 ],\n",
       "       [0.09288187],\n",
       "       [0.10062906],\n",
       "       [0.09858144],\n",
       "       [0.11378029],\n",
       "       [0.12007093],\n",
       "       [0.12226632],\n",
       "       [0.11572237],\n",
       "       [0.12049312],\n",
       "       [0.1169045 ],\n",
       "       [0.11597568],\n",
       "       [0.11804441],\n",
       "       [0.11399139],\n",
       "       [0.10951617],\n",
       "       [0.10495651],\n",
       "       [0.1211264 ],\n",
       "       [0.11795998],\n",
       "       [0.11774888],\n",
       "       [0.10672971],\n",
       "       [0.10905176],\n",
       "       [0.09642827],\n",
       "       [0.09347294],\n",
       "       [0.08507135],\n",
       "       [0.08865997],\n",
       "       [0.07869628],\n",
       "       [0.06624166],\n",
       "       [0.07173014],\n",
       "       [0.07130795],\n",
       "       [0.07713417],\n",
       "       [0.07468547],\n",
       "       [0.06957697],\n",
       "       [0.07768302],\n",
       "       [0.07168792],\n",
       "       [0.0629908 ],\n",
       "       [0.06337077],\n",
       "       [0.05222494],\n",
       "       [0.04373892],\n",
       "       [0.02579583],\n",
       "       [0.027949  ],\n",
       "       [0.03457739],\n",
       "       [0.04061471],\n",
       "       [0.02976442],\n",
       "       [0.03875707],\n",
       "       [0.02866672],\n",
       "       [0.02668243],\n",
       "       [0.02723128],\n",
       "       [0.02516254],\n",
       "       [0.04677869],\n",
       "       [0.03841932],\n",
       "       [0.04074137],\n",
       "       [0.01300346],\n",
       "       [0.01583214],\n",
       "       [0.02955332],\n",
       "       [0.02571139],\n",
       "       [0.01747868],\n",
       "       [0.02537364],\n",
       "       [0.02642911],\n",
       "       [0.0155366 ],\n",
       "       [0.01971629],\n",
       "       [0.01963185],\n",
       "       [0.01659208],\n",
       "       [0.01418559],\n",
       "       [0.01540995],\n",
       "       [0.02659799],\n",
       "       [0.03284641],\n",
       "       [0.02499367],\n",
       "       [0.02406485],\n",
       "       [0.02761125],\n",
       "       [0.01836528],\n",
       "       [0.02431816],\n",
       "       [0.02710462],\n",
       "       [0.0277379 ],\n",
       "       [0.02680909],\n",
       "       [0.04302119],\n",
       "       [0.04395001],\n",
       "       [0.04711644],\n",
       "       [0.05349151],\n",
       "       [0.04867854],\n",
       "       [0.04513215],\n",
       "       [0.04551212],\n",
       "       [0.04572321],\n",
       "       [0.05032509],\n",
       "       [0.05142278],\n",
       "       [0.0601199 ],\n",
       "       [0.06598835],\n",
       "       [0.06527062],\n",
       "       [0.06577725],\n",
       "       [0.06573503],\n",
       "       [0.06915477],\n",
       "       [0.06666385],\n",
       "       [0.06472178],\n",
       "       [0.06269526],\n",
       "       [0.0732078 ],\n",
       "       [0.08114498],\n",
       "       [0.0787385 ],\n",
       "       [0.0829604 ],\n",
       "       [0.08773115],\n",
       "       [0.08220046],\n",
       "       [0.08705564],\n",
       "       [0.07683864],\n",
       "       [0.07734527],\n",
       "       [0.07886515],\n",
       "       [0.08486026],\n",
       "       [0.0916153 ],\n",
       "       [0.09186861],\n",
       "       [0.08236933],\n",
       "       [0.07236342],\n",
       "       [0.06995694],\n",
       "       [0.07088576],\n",
       "       [0.06598835],\n",
       "       [0.064764  ],\n",
       "       [0.06223085],\n",
       "       [0.05914886],\n",
       "       [0.03157984],\n",
       "       [0.01895635],\n",
       "       [0.01435447],\n",
       "       [0.01393228],\n",
       "       [0.02043401],\n",
       "       [0.01625433],\n",
       "       [0.01224352],\n",
       "       [0.01004813],\n",
       "       [0.01034366],\n",
       "       [0.01300346],\n",
       "       [0.00916153],\n",
       "       [0.        ],\n",
       "       [0.00075994],\n",
       "       [0.01494554],\n",
       "       [0.013299  ],\n",
       "       [0.01781643],\n",
       "       [0.01629655],\n",
       "       [0.02060289],\n",
       "       [0.02571139],\n",
       "       [0.03191759],\n",
       "       [0.03917926],\n",
       "       [0.04251457],\n",
       "       [0.04226125],\n",
       "       [0.04019252],\n",
       "       [0.03428185],\n",
       "       [0.03115765],\n",
       "       [0.03200203],\n",
       "       [0.03499958],\n",
       "       [0.03668834],\n",
       "       [0.03630837],\n",
       "       [0.03930592],\n",
       "       [0.03584396],\n",
       "       [0.02955332],\n",
       "       [0.03005995],\n",
       "       [0.02870894],\n",
       "       [0.03043992],\n",
       "       [0.0210673 ],\n",
       "       [0.02009626],\n",
       "       [0.023516  ],\n",
       "       [0.02199612],\n",
       "       [0.02431816],\n",
       "       [0.01291902],\n",
       "       [0.00717724],\n",
       "       [0.01372119],\n",
       "       [0.01714093],\n",
       "       [0.02220721],\n",
       "       [0.02343156],\n",
       "       [0.01963185],\n",
       "       [0.02191168],\n",
       "       [0.02364266],\n",
       "       [0.02676687],\n",
       "       [0.02803344],\n",
       "       [0.02989107],\n",
       "       [0.02756903],\n",
       "       [0.03567508],\n",
       "       [0.03563286],\n",
       "       [0.04006586],\n",
       "       [0.04023474],\n",
       "       [0.04061471],\n",
       "       [0.0383771 ],\n",
       "       [0.03512623],\n",
       "       [0.02955332],\n",
       "       [0.02672465],\n",
       "       [0.0532382 ],\n",
       "       [0.05910665],\n",
       "       [0.0585578 ],\n",
       "       [0.0663261 ],\n",
       "       [0.05969771],\n",
       "       [0.0652284 ],\n",
       "       [0.06556616],\n",
       "       [0.07236342],\n",
       "       [0.07612092],\n",
       "       [0.07797855],\n",
       "       [0.07455881],\n",
       "       [0.07426328],\n",
       "       [0.07531875],\n",
       "       [0.08080723],\n",
       "       [0.08038504],\n",
       "       [0.07970953],\n",
       "       [0.07911847],\n",
       "       [0.0803006 ],\n",
       "       [0.07671198],\n",
       "       [0.07814743],\n",
       "       [0.07468547],\n",
       "       [0.07274339],\n",
       "       [0.07008359],\n",
       "       [0.06957697],\n",
       "       [0.066115  ],\n",
       "       [0.06653719],\n",
       "       [0.06919699],\n",
       "       [0.0734189 ],\n",
       "       [0.07329224],\n",
       "       [0.0760787 ],\n",
       "       [0.06408849],\n",
       "       [0.05399814],\n",
       "       [0.06375074],\n",
       "       [0.07434772],\n",
       "       [0.09047539],\n",
       "       [0.10651862],\n",
       "       [0.10377438],\n",
       "       [0.09811703],\n",
       "       [0.09807481],\n",
       "       [0.09799037],\n",
       "       [0.10250781],\n",
       "       [0.09444398],\n",
       "       [0.0951617 ],\n",
       "       [0.0960483 ],\n",
       "       [0.09967914],\n",
       "       [0.09220637],\n",
       "       [0.09587942],\n",
       "       [0.09364181],\n",
       "       [0.09566833],\n",
       "       [0.09587942],\n",
       "       [0.09942582],\n",
       "       [0.10014354],\n",
       "       [0.10854513],\n",
       "       [0.10960061],\n",
       "       [0.11399139],\n",
       "       [0.1124715 ],\n",
       "       [0.11521574],\n",
       "       [0.11487799],\n",
       "       [0.11454023],\n",
       "       [0.11306257],\n",
       "       [0.11280925],\n",
       "       [0.11086718],\n",
       "       [0.11530018],\n",
       "       [0.11783332],\n",
       "       [0.10660306],\n",
       "       [0.10191674],\n",
       "       [0.0987081 ],\n",
       "       [0.09794816],\n",
       "       [0.08929325],\n",
       "       [0.08971544],\n",
       "       [0.08228489],\n",
       "       [0.07810521],\n",
       "       [0.0847336 ],\n",
       "       [0.08747784],\n",
       "       [0.08671789],\n",
       "       [0.07367221],\n",
       "       [0.07637423],\n",
       "       [0.06489065],\n",
       "       [0.07080132],\n",
       "       [0.0829604 ],\n",
       "       [0.08279152],\n",
       "       [0.08325593],\n",
       "       [0.09030651],\n",
       "       [0.09060204],\n",
       "       [0.08819556],\n",
       "       [0.09055982],\n",
       "       [0.08963101],\n",
       "       [0.0891666 ],\n",
       "       [0.08519801],\n",
       "       [0.08084945],\n",
       "       [0.08258043],\n",
       "       [0.07924512],\n",
       "       [0.08279152],\n",
       "       [0.08735118],\n",
       "       [0.09195305],\n",
       "       [0.09967914],\n",
       "       [0.0969349 ],\n",
       "       [0.1049143 ],\n",
       "       [0.1049143 ],\n",
       "       [0.10757409],\n",
       "       [0.10820738],\n",
       "       [0.11103606],\n",
       "       [0.11234485],\n",
       "       [0.11280925],\n",
       "       [0.10955839],\n",
       "       [0.11052943],\n",
       "       [0.11365364],\n",
       "       [0.11154268],\n",
       "       [0.11141603],\n",
       "       [0.10757409],\n",
       "       [0.10896732],\n",
       "       [0.10841848],\n",
       "       [0.1109094 ],\n",
       "       [0.11639787],\n",
       "       [0.12095753],\n",
       "       [0.12146416],\n",
       "       [0.12416617],\n",
       "       [0.12205522],\n",
       "       [0.12116862],\n",
       "       [0.12522165],\n",
       "       [0.12517943],\n",
       "       [0.12429283],\n",
       "       [0.12522165],\n",
       "       [0.1255594 ],\n",
       "       [0.12509499],\n",
       "       [0.13315883],\n",
       "       [0.13341214],\n",
       "       [0.13345436],\n",
       "       [0.13210335],\n",
       "       [0.13092122],\n",
       "       [0.1621633 ],\n",
       "       [0.16123448],\n",
       "       [0.16355653],\n",
       "       [0.16866503],\n",
       "       [0.17390019],\n",
       "       [0.17605336],\n",
       "       [0.17765769],\n",
       "       [0.17639112],\n",
       "       [0.18133074],\n",
       "       [0.18863464],\n",
       "       [0.19070337],\n",
       "       [0.19000676],\n",
       "       [0.19158997],\n",
       "       [0.19572743],\n",
       "       [0.19745841],\n",
       "       [0.19500971],\n",
       "       [0.19555856],\n",
       "       [0.19669847],\n",
       "       [0.19695179],\n",
       "       [0.20877311],\n",
       "       [0.20526894],\n",
       "       [0.2087309 ],\n",
       "       [0.20687326],\n",
       "       [0.2076332 ],\n",
       "       [0.20543781],\n",
       "       [0.2040868 ],\n",
       "       [0.20602888],\n",
       "       [0.20628219],\n",
       "       [0.20539559],\n",
       "       [0.21160179],\n",
       "       [0.21257283],\n",
       "       [0.2096175 ],\n",
       "       [0.21582369],\n",
       "       [0.20898421],\n",
       "       [0.21565482],\n",
       "       [0.21354387],\n",
       "       [0.21236173],\n",
       "       [0.21337499],\n",
       "       [0.22570295],\n",
       "       [0.22705396],\n",
       "       [0.22625179],\n",
       "       [0.22511188],\n",
       "       [0.22528076],\n",
       "       [0.22979819],\n",
       "       [0.22663177],\n",
       "       [0.22511188],\n",
       "       [0.22376087],\n",
       "       [0.22304315],\n",
       "       [0.21654142],\n",
       "       [0.21725914],\n",
       "       [0.21409271],\n",
       "       [0.2173858 ],\n",
       "       [0.214726  ],\n",
       "       [0.21253061],\n",
       "       [0.21996116],\n",
       "       [0.21924343],\n",
       "       [0.22502744],\n",
       "       [0.22878494],\n",
       "       [0.22519632],\n",
       "       [0.22566073],\n",
       "       [0.22506966],\n",
       "       [0.23743984],\n",
       "       [0.24136621],\n",
       "       [0.23946635],\n",
       "       [0.23722874],\n",
       "       [0.24748797],\n",
       "       [0.26458668],\n",
       "       [0.26872414],\n",
       "       [0.26564215],\n",
       "       [0.26855526],\n",
       "       [0.27763236],\n",
       "       [0.2759436 ],\n",
       "       [0.27497256],\n",
       "       [0.25293422],\n",
       "       [0.26260238],\n",
       "       [0.26479777],\n",
       "       [0.26872414],\n",
       "       [0.26792198],\n",
       "       [0.2659799 ],\n",
       "       [0.26821751],\n",
       "       [0.26711982],\n",
       "       [0.26737313],\n",
       "       [0.2635312 ],\n",
       "       [0.2653044 ],\n",
       "       [0.27488812],\n",
       "       [0.26847083],\n",
       "       [0.27066622],\n",
       "       [0.27455037],\n",
       "       [0.27294604],\n",
       "       [0.24757241],\n",
       "       [0.23254243],\n",
       "       [0.23748206],\n",
       "       [0.23144474],\n",
       "       [0.22777168],\n",
       "       [0.21924343],\n",
       "       [0.23642658],\n",
       "       [0.23081145],\n",
       "       [0.23444229],\n",
       "       [0.23342903],\n",
       "       [0.23617327],\n",
       "       [0.23423119],\n",
       "       [0.22540741],\n",
       "       [0.23427341],\n",
       "       [0.22519632],\n",
       "       [0.22663177],\n",
       "       [0.22443638],\n",
       "       [0.2269273 ],\n",
       "       [0.22118551],\n",
       "       [0.22730727],\n",
       "       [0.23102254],\n",
       "       [0.23300684],\n",
       "       [0.23389344],\n",
       "       [0.2424639 ],\n",
       "       [0.24782572],\n",
       "       [0.25002111],\n",
       "       [0.2522165 ],\n",
       "       [0.25618509],\n",
       "       [0.25331419],\n",
       "       [0.25301866],\n",
       "       [0.26070252],\n",
       "       [0.26344676],\n",
       "       [0.26648653],\n",
       "       [0.25424301],\n",
       "       [0.2497678 ],\n",
       "       [0.24651693],\n",
       "       [0.25208984],\n",
       "       [0.28202314],\n",
       "       [0.27539475],\n",
       "       [0.27885671],\n",
       "       [0.28907371],\n",
       "       [0.29443553],\n",
       "       [0.298573  ],\n",
       "       [0.27433927],\n",
       "       [0.28345858],\n",
       "       [0.29346449],\n",
       "       [0.30085282],\n",
       "       [0.29810859],\n",
       "       [0.28506291],\n",
       "       [0.28354302],\n",
       "       [0.28231867],\n",
       "       [0.29316896],\n",
       "       [0.29401334],\n",
       "       [0.29101579],\n",
       "       [0.29350671],\n",
       "       [0.30030398],\n",
       "       [0.30638352],\n",
       "       [0.30824116],\n",
       "       [0.31098539],\n",
       "       [0.31119649],\n",
       "       [0.30287934],\n",
       "       [0.30216161],\n",
       "       [0.29941738],\n",
       "       [0.28831377],\n",
       "       [0.30043063],\n",
       "       [0.29772862],\n",
       "       [0.29262011],\n",
       "       [0.28683611],\n",
       "       [0.29359115],\n",
       "       [0.28848265],\n",
       "       [0.28873596],\n",
       "       [0.2775057 ],\n",
       "       [0.266191  ],\n",
       "       [0.25985814],\n",
       "       [0.25420079],\n",
       "       [0.26513552],\n",
       "       [0.2697374 ],\n",
       "       [0.26572659],\n",
       "       [0.26927299],\n",
       "       [0.2679642 ],\n",
       "       [0.27079287],\n",
       "       [0.26657097],\n",
       "       [0.27463481],\n",
       "       [0.27425483],\n",
       "       [0.27653466],\n",
       "       [0.27678798],\n",
       "       [0.27953221],\n",
       "       [0.27721017],\n",
       "       [0.28138985],\n",
       "       [0.29359115],\n",
       "       [0.29608207],\n",
       "       [0.29308452],\n",
       "       [0.27712573],\n",
       "       [0.27826564],\n",
       "       [0.27792789],\n",
       "       [0.28185426],\n",
       "       [0.27894115],\n",
       "       [0.28316305],\n",
       "       [0.30697458],\n",
       "       [0.32246897],\n",
       "       [0.33226378],\n",
       "       [0.32318669],\n",
       "       [0.32833741],\n",
       "       [0.34687157],\n",
       "       [0.3542599 ],\n",
       "       [0.35662417],\n",
       "       [0.36266149],\n",
       "       [0.3611416 ],\n",
       "       [0.3560331 ],\n",
       "       [0.35307777],\n",
       "       [0.34197416],\n",
       "       [0.33243266],\n",
       "       [0.34096091],\n",
       "       [0.3369501 ],\n",
       "       [0.33623237],\n",
       "       [0.34957359],\n",
       "       [0.35725745],\n",
       "       [0.35729967],\n",
       "       [0.3535844 ],\n",
       "       [0.34927805],\n",
       "       [0.33412142],\n",
       "       [0.34412733],\n",
       "       [0.34074981],\n",
       "       [0.33547243],\n",
       "       [0.33479693],\n",
       "       [0.33213713],\n",
       "       [0.33344592],\n",
       "       [0.33365701],\n",
       "       [0.34758929],\n",
       "       [0.34349405],\n",
       "       [0.34590053],\n",
       "       [0.34568944],\n",
       "       [0.35307777],\n",
       "       [0.36342143],\n",
       "       [0.35548425],\n",
       "       [0.35468209],\n",
       "       [0.35746855],\n",
       "       [0.35746855],\n",
       "       [0.3387233 ],\n",
       "       [0.33884995],\n",
       "       [0.34087647],\n",
       "       [0.33306595],\n",
       "       [0.34585831],\n",
       "       [0.34573166],\n",
       "       [0.34910918],\n",
       "       [0.35742633],\n",
       "       [0.35468209],\n",
       "       [0.35459765],\n",
       "       [0.35442878],\n",
       "       [0.35860846],\n",
       "       [0.36625011],\n",
       "       [0.36245039],\n",
       "       [0.37473613],\n",
       "       [0.37541164],\n",
       "       [0.37203411],\n",
       "       [0.36587013],\n",
       "       [0.36603901],\n",
       "       [0.35413324],\n",
       "       [0.34100312],\n",
       "       [0.34269189],\n",
       "       [0.32770413],\n",
       "       [0.32352444],\n",
       "       [0.32546652],\n",
       "       [0.32694419],\n",
       "       [0.29620873],\n",
       "       [0.2792789 ],\n",
       "       [0.30689015],\n",
       "       [0.2921557 ],\n",
       "       [0.27362155],\n",
       "       [0.27894115],\n",
       "       [0.30553914],\n",
       "       [0.31242084],\n",
       "       [0.32521321],\n",
       "       [0.3489403 ],\n",
       "       [0.34657604],\n",
       "       [0.34412733],\n",
       "       [0.34083425],\n",
       "       [0.34687157],\n",
       "       [0.35953728],\n",
       "       [0.37418728],\n",
       "       [0.37173858],\n",
       "       [0.37059867],\n",
       "       [0.35742633],\n",
       "       [0.36253483],\n",
       "       [0.36511019],\n",
       "       [0.36447691],\n",
       "       [0.35755298],\n",
       "       [0.36561682],\n",
       "       [0.37845141],\n",
       "       [0.38579752],\n",
       "       [0.37840919],\n",
       "       [0.37194967],\n",
       "       [0.37283627],\n",
       "       [0.37017648],\n",
       "       [0.3586929 ],\n",
       "       [0.35843958],\n",
       "       [0.34167863],\n",
       "       [0.33146162],\n",
       "       [0.31495398],\n",
       "       [0.34801148],\n",
       "       [0.32930845],\n",
       "       [0.32145571],\n",
       "       [0.32694419],\n",
       "       [0.32230009],\n",
       "       [0.32951955],\n",
       "       [0.34311408],\n",
       "       [0.34813814],\n",
       "       [0.32947733],\n",
       "       [0.33652791],\n",
       "       [0.350038  ],\n",
       "       [0.34661826],\n",
       "       [0.35379549],\n",
       "       [0.35628641],\n",
       "       [0.36088829],\n",
       "       [0.37110529],\n",
       "       [0.36941653],\n",
       "       [0.34813814],\n",
       "       [0.31824707],\n",
       "       [0.31622055],\n",
       "       [0.30651017],\n",
       "       [0.30950773],\n",
       "       [0.31191421],\n",
       "       [0.30389259],\n",
       "       [0.31630499],\n",
       "       [0.3325171 ],\n",
       "       [0.36405472],\n",
       "       [0.36540572],\n",
       "       [0.39470573],\n",
       "       [0.40032086],\n",
       "       [0.40407836],\n",
       "       [0.40960905],\n",
       "       [0.42092375],\n",
       "       [0.41480199],\n",
       "       [0.41294436],\n",
       "       [0.4057249 ],\n",
       "       [0.41307101],\n",
       "       [0.40804695],\n",
       "       [0.40517605],\n",
       "       [0.41074897],\n",
       "       [0.40876467],\n",
       "       [0.41383095],\n",
       "       [0.41294436],\n",
       "       [0.41475977],\n",
       "       [0.41188888],\n",
       "       [0.41020012],\n",
       "       [0.40754032],\n",
       "       [0.42176813],\n",
       "       [0.42848096],\n",
       "       [0.43472938],\n",
       "       [0.43755805],\n",
       "       [0.43536266],\n",
       "       [0.42793211],\n",
       "       [0.42594782],\n",
       "       [0.43038082],\n",
       "       [0.42371021],\n",
       "       [0.4241324 ],\n",
       "       [0.41585747],\n",
       "       [0.41543528],\n",
       "       [0.40255847],\n",
       "       [0.40597821],\n",
       "       [0.40158744],\n",
       "       [0.39930761],\n",
       "       [0.38769737],\n",
       "       [0.39723888],\n",
       "       [0.39609896],\n",
       "       [0.40175631],\n",
       "       [0.40010977],\n",
       "       [0.40884911],\n",
       "       [0.3950857 ],\n",
       "       [0.40133412],\n",
       "       [0.41218441],\n",
       "       [0.42320358],\n",
       "       [0.42223254],\n",
       "       [0.41180444],\n",
       "       [0.42510344],\n",
       "       [0.42637001],\n",
       "       [0.42459681],\n",
       "       [0.42687664],\n",
       "       [0.42244364],\n",
       "       [0.42869205],\n",
       "       [0.42683442],\n",
       "       [0.42755214],\n",
       "       [0.43342059],\n",
       "       [0.44110445],\n",
       "       [0.43852909],\n",
       "       [0.42489234],\n",
       "       [0.42037491],\n",
       "       [0.42197923],\n",
       "       [0.46930676],\n",
       "       [0.49417377],\n",
       "       [0.49670692],\n",
       "       [0.50126657],\n",
       "       [0.49299164],\n",
       "       [0.49358271],\n",
       "       [0.50046441],\n",
       "       [0.49476484],\n",
       "       [0.50042219],\n",
       "       [0.50413747],\n",
       "       [0.5062062 ],\n",
       "       [0.51920966],\n",
       "       [0.53719497],\n",
       "       [0.52824453],\n",
       "       [0.52647133]])"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "324bd6c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "# convert an array of values into a dataset matrix\n",
    "def create_dataset(dataset, time_step=1):\n",
    "\tdataX, dataY = [], []\n",
    "\tfor i in range(len(dataset)-time_step-1):\n",
    "\t\ta = dataset[i:(i+time_step), 0]   ###i=0, 0,1,2,3-----99   100 \n",
    "\t\tdataX.append(a)\n",
    "\t\tdataY.append(dataset[i + time_step, 0])\n",
    "\treturn numpy.array(dataX), numpy.array(dataY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "63277932",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshape into X=t,t+1,t+2,t+3 and Y=t+4\n",
    "time_step = 100\n",
    "X_train, y_train = create_dataset(train_data, time_step)\n",
    "X_test, ytest = create_dataset(test_data, time_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "db35e44e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(716, 100)\n",
      "(716,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None, None)"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(X_train.shape), print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "354be9f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(340, 100)\n",
      "(340,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None, None)"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(X_test.shape), print(ytest.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "ea789cac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshape input to be [samples, time steps, features] which is required for LSTM\n",
    "X_train =X_train.reshape(X_train.shape[0],X_train.shape[1] , 1)\n",
    "X_test = X_test.reshape(X_test.shape[0],X_test.shape[1] , 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "02a7a10f",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Create the Stacked LSTM model\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.layers import Bidirectional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "65f4569f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from keras.optimizers import RMSprop\n",
    "# \n",
    "# from keras.optimizers import RMSprop\n",
    "# def bidirectional_model(learn_rate):\n",
    "#     model = Sequential()\n",
    "#     model.add(Bidirectional(LSTM(50),input_shape=(100,1)))\n",
    "#     model.add(Dense(1))\n",
    "#     optimizer = RMSprop(lr=learn_rate)\n",
    "#     model.compile(loss='mean_squared_error', optimizer=optimizer)\n",
    "#     return model\n",
    "\n",
    "# def fixed_model(learn_rate):\n",
    "#     model = Sequential()\n",
    "#     model.add(LSTM(50,return_sequences=True,input_shape=(100,1)))\n",
    "#     model.add(Dense(1))\n",
    "#     optimizer = RMSprop(lr=learn_rate)\n",
    "#     model.compile(loss='mean_squared_error', optimizer=optimizer)\n",
    "#     return model\n",
    "# def stacked_model(learn_rate):\n",
    "#     model = Sequential()\n",
    "#     model.add(LSTM(50,return_sequences=True,input_shape=(100,1)))\n",
    "#     model.add(LSTM(50))\n",
    "#     model.add(Dense(1))\n",
    "#     optimizer = RMSprop(lr=learn_rate)\n",
    "#     model.compile(loss='mean_squared_error', optimizer=optimizer)\n",
    "#     return model\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "9a2d2b19",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.optimizers import RMSprop\n",
    "def bidirectional_model(learn_rate):\n",
    "    model = Sequential()\n",
    "    model.add(Bidirectional(LSTM(50),input_shape=(100,1)))\n",
    "    model.add(Dense(1))\n",
    "    optimizer = RMSprop(lr=learn_rate)\n",
    "    model.compile(loss='mean_squared_error', optimizer=optimizer)\n",
    "    return model\n",
    "\n",
    "\n",
    "def stacked_model(learn_rate):\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(50,return_sequences=True,input_shape=(100,1)))\n",
    "    model.add(LSTM(50))\n",
    "    model.add(Dense(1))\n",
    "    optimizer = RMSprop(lr=learn_rate)\n",
    "    model.compile(loss='mean_squared_error', optimizer=optimizer)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "83e9ff58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_11\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_13 (LSTM)              (None, 100, 50)           10400     \n",
      "                                                                 \n",
      " dense_11 (Dense)            (None, 100, 1)            51        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 10,451\n",
      "Trainable params: 10,451\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Aviral\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\optimizers\\optimizer_v2\\rmsprop.py:140: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "# model=fixed_model(0.001)\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "bbc9c102",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Eshan stacked\n",
    "\n",
    "# from keras.optimizers import RMSprop\n",
    "# model = Sequential()\n",
    "# model.add(LSTM(50,return_sequences=True,input_shape=(100,1)))\n",
    "# model.add(LSTM(50))\n",
    "# model.add(Dense(1))\n",
    "# optimizer = RMSprop(lr=0.001)\n",
    "# model.compile(loss='mean_squared_error', optimizer=optimizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "4f9602cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Krish Naik\n",
    "\n",
    "# model=Sequential()\n",
    "# model.add(LSTM(50,return_sequences=True,input_shape=(100,1)))\n",
    "# model.add(LSTM(50,return_sequences=True))\n",
    "# model.add(LSTM(50))\n",
    "# model.add(Dense(1))\n",
    "# model.compile(loss='mean_squared_error',optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "ba21ba2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "models =[]\n",
    "# models.append((\"Fixed\",fixed_model))\n",
    "models.append((\"Bidirectional\",bidirectional_model))\n",
    "models.append((\"Stacked\",stacked_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "b6fe7e09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_12\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " bidirectional_2 (Bidirectio  (None, 100)              20800     \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " dense_12 (Dense)            (None, 1)                 101       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 20,901\n",
      "Trainable params: 20,901\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "12/12 [==============================] - 4s 102ms/step - loss: 0.5804 - val_loss: 0.0701\n",
      "Epoch 2/100\n",
      "12/12 [==============================] - 1s 48ms/step - loss: 0.0034 - val_loss: 0.0250\n",
      "Epoch 3/100\n",
      "12/12 [==============================] - 1s 48ms/step - loss: 0.0024 - val_loss: 0.0063\n",
      "Epoch 4/100\n",
      "12/12 [==============================] - 1s 49ms/step - loss: 0.0027 - val_loss: 0.0061\n",
      "Epoch 5/100\n",
      "12/12 [==============================] - 1s 48ms/step - loss: 0.0038 - val_loss: 0.0094\n",
      "Epoch 6/100\n",
      "12/12 [==============================] - 1s 48ms/step - loss: 0.0039 - val_loss: 0.0175\n",
      "Epoch 7/100\n",
      "12/12 [==============================] - 1s 48ms/step - loss: 0.0035 - val_loss: 0.0208\n",
      "Epoch 8/100\n",
      "12/12 [==============================] - 1s 49ms/step - loss: 0.0032 - val_loss: 0.0223\n",
      "Epoch 9/100\n",
      "12/12 [==============================] - 1s 49ms/step - loss: 0.0037 - val_loss: 0.0070\n",
      "Epoch 10/100\n",
      "12/12 [==============================] - 1s 50ms/step - loss: 0.0054 - val_loss: 0.0090\n",
      "Epoch 11/100\n",
      "12/12 [==============================] - 1s 50ms/step - loss: 0.0012 - val_loss: 0.0442\n",
      "Epoch 12/100\n",
      "12/12 [==============================] - 1s 49ms/step - loss: 0.0028 - val_loss: 0.0075\n",
      "Epoch 13/100\n",
      "12/12 [==============================] - 1s 48ms/step - loss: 0.0027 - val_loss: 0.1026\n",
      "Epoch 14/100\n",
      "12/12 [==============================] - 1s 48ms/step - loss: 0.0028 - val_loss: 0.0044\n",
      "Epoch 15/100\n",
      "12/12 [==============================] - 1s 48ms/step - loss: 0.0020 - val_loss: 0.0145\n",
      "Epoch 16/100\n",
      "12/12 [==============================] - 1s 49ms/step - loss: 0.0019 - val_loss: 0.0364\n",
      "Epoch 17/100\n",
      "12/12 [==============================] - 1s 49ms/step - loss: 0.0012 - val_loss: 0.0132\n",
      "Epoch 18/100\n",
      "12/12 [==============================] - 1s 49ms/step - loss: 0.0032 - val_loss: 0.0077\n",
      "Epoch 19/100\n",
      "12/12 [==============================] - 1s 49ms/step - loss: 1.9115e-04 - val_loss: 0.0031\n",
      "Epoch 20/100\n",
      "12/12 [==============================] - 1s 50ms/step - loss: 0.0017 - val_loss: 0.0042\n",
      "Epoch 21/100\n",
      "12/12 [==============================] - 1s 49ms/step - loss: 1.8722e-04 - val_loss: 0.0149\n",
      "Epoch 22/100\n",
      "12/12 [==============================] - 1s 50ms/step - loss: 0.0027 - val_loss: 0.0040\n",
      "Epoch 23/100\n",
      "12/12 [==============================] - 1s 50ms/step - loss: 8.0208e-04 - val_loss: 0.0111\n",
      "Epoch 24/100\n",
      "12/12 [==============================] - 1s 49ms/step - loss: 0.0011 - val_loss: 0.0277\n",
      "Epoch 25/100\n",
      "12/12 [==============================] - 1s 51ms/step - loss: 0.0010 - val_loss: 0.0093\n",
      "Epoch 26/100\n",
      "12/12 [==============================] - 1s 50ms/step - loss: 7.1185e-04 - val_loss: 0.0166\n",
      "Epoch 27/100\n",
      "12/12 [==============================] - 1s 50ms/step - loss: 0.0020 - val_loss: 0.0028\n",
      "Epoch 28/100\n",
      "12/12 [==============================] - 1s 51ms/step - loss: 1.3197e-04 - val_loss: 0.0061\n",
      "Epoch 29/100\n",
      "12/12 [==============================] - 1s 50ms/step - loss: 0.0012 - val_loss: 0.0076\n",
      "Epoch 30/100\n",
      "12/12 [==============================] - 1s 50ms/step - loss: 4.4359e-04 - val_loss: 0.0236\n",
      "Epoch 31/100\n",
      "12/12 [==============================] - 1s 51ms/step - loss: 0.0011 - val_loss: 0.0018\n",
      "Epoch 32/100\n",
      "12/12 [==============================] - 1s 50ms/step - loss: 6.0746e-04 - val_loss: 0.0052\n",
      "Epoch 33/100\n",
      "12/12 [==============================] - 1s 50ms/step - loss: 7.5509e-04 - val_loss: 0.0057\n",
      "Epoch 34/100\n",
      "12/12 [==============================] - 1s 51ms/step - loss: 4.4335e-04 - val_loss: 0.0031\n",
      "Epoch 35/100\n",
      "12/12 [==============================] - 1s 51ms/step - loss: 8.4549e-04 - val_loss: 0.0031\n",
      "Epoch 36/100\n",
      "12/12 [==============================] - 1s 52ms/step - loss: 4.8588e-04 - val_loss: 0.0041\n",
      "Epoch 37/100\n",
      "12/12 [==============================] - 1s 51ms/step - loss: 0.0012 - val_loss: 0.0011\n",
      "Epoch 38/100\n",
      "12/12 [==============================] - 1s 52ms/step - loss: 2.9589e-04 - val_loss: 0.0099\n",
      "Epoch 39/100\n",
      "12/12 [==============================] - 1s 54ms/step - loss: 5.5446e-04 - val_loss: 0.0028\n",
      "Epoch 40/100\n",
      "12/12 [==============================] - 1s 51ms/step - loss: 8.5789e-04 - val_loss: 0.0082\n",
      "Epoch 41/100\n",
      "12/12 [==============================] - 1s 51ms/step - loss: 1.3696e-04 - val_loss: 9.2450e-04\n",
      "Epoch 42/100\n",
      "12/12 [==============================] - 1s 52ms/step - loss: 7.1562e-04 - val_loss: 8.8900e-04\n",
      "Epoch 43/100\n",
      "12/12 [==============================] - 1s 51ms/step - loss: 4.8951e-04 - val_loss: 0.0012\n",
      "Epoch 44/100\n",
      "12/12 [==============================] - 1s 52ms/step - loss: 6.7925e-04 - val_loss: 0.0076\n",
      "Epoch 45/100\n",
      "12/12 [==============================] - 1s 52ms/step - loss: 5.4103e-04 - val_loss: 0.0042\n",
      "Epoch 46/100\n",
      "12/12 [==============================] - 1s 57ms/step - loss: 3.4156e-04 - val_loss: 0.0020\n",
      "Epoch 47/100\n",
      "12/12 [==============================] - 1s 56ms/step - loss: 7.1322e-04 - val_loss: 0.0015\n",
      "Epoch 48/100\n",
      "12/12 [==============================] - 1s 58ms/step - loss: 3.5587e-04 - val_loss: 0.0086\n",
      "Epoch 49/100\n",
      "12/12 [==============================] - 1s 55ms/step - loss: 4.1132e-04 - val_loss: 0.0100\n",
      "Epoch 50/100\n",
      "12/12 [==============================] - 1s 53ms/step - loss: 6.2144e-04 - val_loss: 0.0038\n",
      "Epoch 51/100\n",
      "12/12 [==============================] - 1s 63ms/step - loss: 3.0694e-04 - val_loss: 0.0116\n",
      "Epoch 52/100\n",
      "12/12 [==============================] - 1s 64ms/step - loss: 5.6082e-04 - val_loss: 8.7093e-04\n",
      "Epoch 53/100\n",
      "12/12 [==============================] - 1s 70ms/step - loss: 4.8070e-04 - val_loss: 0.0058\n",
      "Epoch 54/100\n",
      "12/12 [==============================] - 1s 66ms/step - loss: 2.8092e-04 - val_loss: 0.0099\n",
      "Epoch 55/100\n",
      "12/12 [==============================] - 1s 64ms/step - loss: 5.1264e-04 - val_loss: 0.0021\n",
      "Epoch 56/100\n",
      "12/12 [==============================] - 1s 58ms/step - loss: 4.4341e-04 - val_loss: 0.0079\n",
      "Epoch 57/100\n",
      "12/12 [==============================] - 1s 65ms/step - loss: 3.4731e-04 - val_loss: 0.0015\n",
      "Epoch 58/100\n",
      "12/12 [==============================] - 1s 64ms/step - loss: 4.0245e-04 - val_loss: 0.0016\n",
      "Epoch 59/100\n",
      "12/12 [==============================] - 1s 55ms/step - loss: 2.1777e-04 - val_loss: 0.0154\n",
      "Epoch 60/100\n",
      "12/12 [==============================] - 1s 54ms/step - loss: 5.1677e-04 - val_loss: 0.0043\n",
      "Epoch 61/100\n",
      "12/12 [==============================] - 1s 54ms/step - loss: 3.5241e-04 - val_loss: 0.0014\n",
      "Epoch 62/100\n",
      "12/12 [==============================] - 1s 54ms/step - loss: 3.9998e-04 - val_loss: 0.0049\n",
      "Epoch 63/100\n",
      "12/12 [==============================] - 1s 61ms/step - loss: 2.7361e-04 - val_loss: 0.0025\n",
      "Epoch 64/100\n",
      "12/12 [==============================] - 1s 57ms/step - loss: 5.1713e-04 - val_loss: 0.0013\n",
      "Epoch 65/100\n",
      "12/12 [==============================] - 1s 62ms/step - loss: 2.8297e-04 - val_loss: 0.0100\n",
      "Epoch 66/100\n",
      "12/12 [==============================] - 1s 57ms/step - loss: 2.5203e-04 - val_loss: 0.0074\n",
      "Epoch 67/100\n",
      "12/12 [==============================] - 1s 61ms/step - loss: 4.9546e-04 - val_loss: 0.0015\n",
      "Epoch 68/100\n",
      "12/12 [==============================] - 1s 70ms/step - loss: 3.8660e-04 - val_loss: 0.0026\n",
      "Epoch 69/100\n",
      "12/12 [==============================] - 1s 57ms/step - loss: 2.4261e-04 - val_loss: 0.0018\n",
      "Epoch 70/100\n",
      "12/12 [==============================] - 1s 54ms/step - loss: 3.2621e-04 - val_loss: 0.0012\n",
      "Epoch 71/100\n",
      "12/12 [==============================] - 1s 54ms/step - loss: 3.9567e-04 - val_loss: 8.2778e-04\n",
      "Epoch 72/100\n",
      "12/12 [==============================] - 1s 53ms/step - loss: 3.3664e-04 - val_loss: 8.9565e-04\n",
      "Epoch 73/100\n",
      "12/12 [==============================] - 1s 54ms/step - loss: 2.4073e-04 - val_loss: 9.5163e-04\n",
      "Epoch 74/100\n",
      "12/12 [==============================] - 1s 54ms/step - loss: 3.3830e-04 - val_loss: 0.0012\n",
      "Epoch 75/100\n",
      "12/12 [==============================] - 1s 55ms/step - loss: 3.6133e-04 - val_loss: 0.0034\n",
      "Epoch 76/100\n",
      "12/12 [==============================] - 1s 55ms/step - loss: 3.0174e-04 - val_loss: 0.0024\n",
      "Epoch 77/100\n",
      "12/12 [==============================] - 1s 53ms/step - loss: 2.5741e-04 - val_loss: 0.0023\n",
      "Epoch 78/100\n",
      "12/12 [==============================] - 1s 54ms/step - loss: 2.6635e-04 - val_loss: 7.6692e-04\n",
      "Epoch 79/100\n",
      "12/12 [==============================] - 1s 56ms/step - loss: 4.1543e-04 - val_loss: 7.9966e-04\n",
      "Epoch 80/100\n",
      "12/12 [==============================] - 1s 58ms/step - loss: 2.2751e-04 - val_loss: 0.0079\n",
      "Epoch 81/100\n",
      "12/12 [==============================] - 1s 58ms/step - loss: 3.9888e-04 - val_loss: 8.5234e-04\n",
      "Epoch 82/100\n",
      "12/12 [==============================] - 1s 120ms/step - loss: 3.3168e-04 - val_loss: 9.5712e-04\n",
      "Epoch 83/100\n",
      "12/12 [==============================] - 1s 58ms/step - loss: 3.2114e-04 - val_loss: 0.0055\n",
      "Epoch 84/100\n",
      "12/12 [==============================] - 1s 56ms/step - loss: 1.8529e-04 - val_loss: 7.4456e-04\n",
      "Epoch 85/100\n",
      "12/12 [==============================] - 1s 52ms/step - loss: 3.4551e-04 - val_loss: 0.0059\n",
      "Epoch 86/100\n",
      "12/12 [==============================] - 1s 51ms/step - loss: 1.5982e-04 - val_loss: 0.0017\n",
      "Epoch 87/100\n",
      "12/12 [==============================] - 1s 52ms/step - loss: 4.0979e-04 - val_loss: 8.0139e-04\n",
      "Epoch 88/100\n",
      "12/12 [==============================] - 1s 54ms/step - loss: 2.9970e-04 - val_loss: 0.0032\n",
      "Epoch 89/100\n",
      "12/12 [==============================] - 1s 54ms/step - loss: 2.7996e-04 - val_loss: 0.0044\n",
      "Epoch 90/100\n",
      "12/12 [==============================] - 1s 54ms/step - loss: 2.8113e-04 - val_loss: 7.0098e-04\n",
      "Epoch 91/100\n",
      "12/12 [==============================] - 1s 54ms/step - loss: 2.7928e-04 - val_loss: 0.0040\n",
      "Epoch 92/100\n",
      "12/12 [==============================] - 1s 54ms/step - loss: 2.4374e-04 - val_loss: 0.0096\n",
      "Epoch 93/100\n",
      "12/12 [==============================] - 1s 54ms/step - loss: 2.3012e-04 - val_loss: 8.6553e-04\n",
      "Epoch 94/100\n",
      "12/12 [==============================] - 1s 54ms/step - loss: 3.7327e-04 - val_loss: 0.0020\n",
      "Epoch 95/100\n",
      "12/12 [==============================] - 1s 55ms/step - loss: 1.6813e-04 - val_loss: 9.9425e-04\n",
      "Epoch 96/100\n",
      "12/12 [==============================] - 1s 54ms/step - loss: 3.3756e-04 - val_loss: 0.0011\n",
      "Epoch 97/100\n",
      "12/12 [==============================] - 1s 54ms/step - loss: 2.5080e-04 - val_loss: 9.4602e-04\n",
      "Epoch 98/100\n",
      "12/12 [==============================] - 1s 58ms/step - loss: 2.8053e-04 - val_loss: 0.0012\n",
      "Epoch 99/100\n",
      "12/12 [==============================] - 1s 64ms/step - loss: 2.6099e-04 - val_loss: 0.0012\n",
      "Epoch 100/100\n",
      "12/12 [==============================] - 1s 60ms/step - loss: 1.9505e-04 - val_loss: 0.0018\n",
      "23/23 [==============================] - 1s 13ms/step\n",
      "11/11 [==============================] - 0s 13ms/step\n",
      "Model: \"sequential_13\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " bidirectional_3 (Bidirectio  (None, 100)              20800     \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " dense_13 (Dense)            (None, 1)                 101       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 20,901\n",
      "Trainable params: 20,901\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Aviral\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\optimizers\\optimizer_v2\\rmsprop.py:140: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 4s 136ms/step - loss: 0.0078 - val_loss: 0.0111\n",
      "Epoch 2/100\n",
      "12/12 [==============================] - 1s 81ms/step - loss: 0.0014 - val_loss: 0.0200\n",
      "Epoch 3/100\n",
      "12/12 [==============================] - 1s 80ms/step - loss: 0.0010 - val_loss: 0.0071\n",
      "Epoch 4/100\n",
      "12/12 [==============================] - 1s 80ms/step - loss: 0.0011 - val_loss: 0.0117\n",
      "Epoch 5/100\n",
      "12/12 [==============================] - 1s 80ms/step - loss: 8.3949e-04 - val_loss: 0.0042\n",
      "Epoch 6/100\n",
      "12/12 [==============================] - 1s 79ms/step - loss: 0.0011 - val_loss: 0.0029\n",
      "Epoch 7/100\n",
      "12/12 [==============================] - 1s 86ms/step - loss: 7.5724e-04 - val_loss: 0.0028\n",
      "Epoch 8/100\n",
      "12/12 [==============================] - 1s 87ms/step - loss: 8.5864e-04 - val_loss: 0.0048\n",
      "Epoch 9/100\n",
      "12/12 [==============================] - 1s 88ms/step - loss: 7.4416e-04 - val_loss: 0.0075\n",
      "Epoch 10/100\n",
      "12/12 [==============================] - 1s 85ms/step - loss: 6.5307e-04 - val_loss: 0.0022\n",
      "Epoch 11/100\n",
      "12/12 [==============================] - 1s 86ms/step - loss: 6.5624e-04 - val_loss: 0.0031\n",
      "Epoch 12/100\n",
      "12/12 [==============================] - 1s 84ms/step - loss: 5.4981e-04 - val_loss: 0.0058\n",
      "Epoch 13/100\n",
      "12/12 [==============================] - 1s 86ms/step - loss: 5.3482e-04 - val_loss: 0.0032\n",
      "Epoch 14/100\n",
      "12/12 [==============================] - 1s 86ms/step - loss: 6.4085e-04 - val_loss: 0.0091\n",
      "Epoch 15/100\n",
      "12/12 [==============================] - 1s 85ms/step - loss: 4.6438e-04 - val_loss: 0.0020\n",
      "Epoch 16/100\n",
      "12/12 [==============================] - 1s 86ms/step - loss: 5.4897e-04 - val_loss: 0.0073\n",
      "Epoch 17/100\n",
      "12/12 [==============================] - 1s 87ms/step - loss: 4.8776e-04 - val_loss: 0.0018\n",
      "Epoch 18/100\n",
      "12/12 [==============================] - 1s 92ms/step - loss: 3.3060e-04 - val_loss: 0.0151\n",
      "Epoch 19/100\n",
      "12/12 [==============================] - 1s 85ms/step - loss: 4.9262e-04 - val_loss: 0.0018\n",
      "Epoch 20/100\n",
      "12/12 [==============================] - 1s 84ms/step - loss: 5.0208e-04 - val_loss: 0.0015\n",
      "Epoch 21/100\n",
      "12/12 [==============================] - 1s 85ms/step - loss: 3.5771e-04 - val_loss: 0.0018\n",
      "Epoch 22/100\n",
      "12/12 [==============================] - 1s 84ms/step - loss: 3.6058e-04 - val_loss: 0.0037\n",
      "Epoch 23/100\n",
      "12/12 [==============================] - 1s 86ms/step - loss: 3.8685e-04 - val_loss: 0.0014\n",
      "Epoch 24/100\n",
      "12/12 [==============================] - 1s 84ms/step - loss: 4.0134e-04 - val_loss: 0.0015\n",
      "Epoch 25/100\n",
      "12/12 [==============================] - 1s 83ms/step - loss: 3.9225e-04 - val_loss: 0.0031\n",
      "Epoch 26/100\n",
      "12/12 [==============================] - 1s 84ms/step - loss: 3.3981e-04 - val_loss: 0.0033\n",
      "Epoch 27/100\n",
      "12/12 [==============================] - 1s 84ms/step - loss: 3.0827e-04 - val_loss: 0.0014\n",
      "Epoch 28/100\n",
      "12/12 [==============================] - 1s 83ms/step - loss: 2.6915e-04 - val_loss: 0.0013\n",
      "Epoch 29/100\n",
      "12/12 [==============================] - 1s 85ms/step - loss: 3.2398e-04 - val_loss: 0.0013\n",
      "Epoch 30/100\n",
      "12/12 [==============================] - 1s 83ms/step - loss: 3.4915e-04 - val_loss: 0.0012\n",
      "Epoch 31/100\n",
      "12/12 [==============================] - 1s 85ms/step - loss: 3.2044e-04 - val_loss: 0.0013\n",
      "Epoch 32/100\n",
      "12/12 [==============================] - 1s 84ms/step - loss: 3.3118e-04 - val_loss: 0.0013\n",
      "Epoch 33/100\n",
      "12/12 [==============================] - 1s 84ms/step - loss: 2.9823e-04 - val_loss: 0.0024\n",
      "Epoch 34/100\n",
      "12/12 [==============================] - 1s 84ms/step - loss: 2.7919e-04 - val_loss: 0.0024\n",
      "Epoch 35/100\n",
      "12/12 [==============================] - 1s 84ms/step - loss: 1.6157e-04 - val_loss: 0.0025\n",
      "Epoch 36/100\n",
      "12/12 [==============================] - 1s 85ms/step - loss: 3.5093e-04 - val_loss: 0.0050\n",
      "Epoch 37/100\n",
      "12/12 [==============================] - 1s 86ms/step - loss: 2.8385e-04 - val_loss: 0.0021\n",
      "Epoch 38/100\n",
      "12/12 [==============================] - 1s 92ms/step - loss: 2.6136e-04 - val_loss: 0.0011\n",
      "Epoch 39/100\n",
      "12/12 [==============================] - 1s 92ms/step - loss: 2.6404e-04 - val_loss: 0.0017\n",
      "Epoch 40/100\n",
      "12/12 [==============================] - 1s 87ms/step - loss: 1.7574e-04 - val_loss: 0.0016\n",
      "Epoch 41/100\n",
      "12/12 [==============================] - 1s 90ms/step - loss: 2.6534e-04 - val_loss: 0.0022\n",
      "Epoch 42/100\n",
      "12/12 [==============================] - 1s 88ms/step - loss: 2.5082e-04 - val_loss: 0.0019\n",
      "Epoch 43/100\n",
      "12/12 [==============================] - 1s 88ms/step - loss: 2.4215e-04 - val_loss: 0.0022\n",
      "Epoch 44/100\n",
      "12/12 [==============================] - 1s 88ms/step - loss: 2.1595e-04 - val_loss: 0.0018\n",
      "Epoch 45/100\n",
      "12/12 [==============================] - 1s 86ms/step - loss: 2.1033e-04 - val_loss: 0.0029\n",
      "Epoch 46/100\n",
      "12/12 [==============================] - 1s 86ms/step - loss: 2.0368e-04 - val_loss: 0.0017\n",
      "Epoch 47/100\n",
      "12/12 [==============================] - 1s 86ms/step - loss: 2.5897e-04 - val_loss: 0.0018\n",
      "Epoch 48/100\n",
      "12/12 [==============================] - 1s 85ms/step - loss: 1.9369e-04 - val_loss: 9.9892e-04\n",
      "Epoch 49/100\n",
      "12/12 [==============================] - 1s 84ms/step - loss: 2.5030e-04 - val_loss: 0.0023\n",
      "Epoch 50/100\n",
      "12/12 [==============================] - 1s 86ms/step - loss: 1.8521e-04 - val_loss: 0.0025\n",
      "Epoch 51/100\n",
      "12/12 [==============================] - 1s 84ms/step - loss: 2.1812e-04 - val_loss: 9.6124e-04\n",
      "Epoch 52/100\n",
      "12/12 [==============================] - 1s 85ms/step - loss: 2.2453e-04 - val_loss: 9.8416e-04\n",
      "Epoch 53/100\n",
      "12/12 [==============================] - 1s 85ms/step - loss: 2.1737e-04 - val_loss: 0.0010\n",
      "Epoch 54/100\n",
      "12/12 [==============================] - 1s 85ms/step - loss: 2.1509e-04 - val_loss: 9.6648e-04\n",
      "Epoch 55/100\n",
      "12/12 [==============================] - 1s 84ms/step - loss: 2.2505e-04 - val_loss: 0.0013\n",
      "Epoch 56/100\n",
      "12/12 [==============================] - 1s 85ms/step - loss: 2.1769e-04 - val_loss: 0.0018\n",
      "Epoch 57/100\n",
      "12/12 [==============================] - 1s 84ms/step - loss: 2.3782e-04 - val_loss: 0.0011\n",
      "Epoch 58/100\n",
      "12/12 [==============================] - 1s 86ms/step - loss: 1.8709e-04 - val_loss: 0.0010\n",
      "Epoch 59/100\n",
      "12/12 [==============================] - 1s 85ms/step - loss: 1.5659e-04 - val_loss: 0.0019\n",
      "Epoch 60/100\n",
      "12/12 [==============================] - 1s 85ms/step - loss: 2.1440e-04 - val_loss: 9.3653e-04\n",
      "Epoch 61/100\n",
      "12/12 [==============================] - 1s 85ms/step - loss: 2.1178e-04 - val_loss: 9.1710e-04\n",
      "Epoch 62/100\n",
      "12/12 [==============================] - 1s 87ms/step - loss: 2.2758e-04 - val_loss: 0.0011\n",
      "Epoch 63/100\n",
      "12/12 [==============================] - 1s 85ms/step - loss: 1.9642e-04 - val_loss: 8.6675e-04\n",
      "Epoch 64/100\n",
      "12/12 [==============================] - 1s 85ms/step - loss: 1.3940e-04 - val_loss: 0.0012\n",
      "Epoch 65/100\n",
      "12/12 [==============================] - 1s 86ms/step - loss: 1.4943e-04 - val_loss: 8.9466e-04\n",
      "Epoch 66/100\n",
      "12/12 [==============================] - 1s 86ms/step - loss: 2.0994e-04 - val_loss: 0.0013\n",
      "Epoch 67/100\n",
      "12/12 [==============================] - 1s 84ms/step - loss: 1.4462e-04 - val_loss: 0.0010\n",
      "Epoch 68/100\n",
      "12/12 [==============================] - 1s 85ms/step - loss: 1.7093e-04 - val_loss: 0.0023\n",
      "Epoch 69/100\n",
      "12/12 [==============================] - 1s 85ms/step - loss: 2.0746e-04 - val_loss: 8.3580e-04\n",
      "Epoch 70/100\n",
      "12/12 [==============================] - 1s 86ms/step - loss: 1.7899e-04 - val_loss: 0.0017\n",
      "Epoch 71/100\n",
      "12/12 [==============================] - 1s 86ms/step - loss: 1.5742e-04 - val_loss: 8.7856e-04\n",
      "Epoch 72/100\n",
      "12/12 [==============================] - 1s 86ms/step - loss: 1.5773e-04 - val_loss: 0.0028\n",
      "Epoch 73/100\n",
      "12/12 [==============================] - 1s 85ms/step - loss: 2.0107e-04 - val_loss: 0.0023\n",
      "Epoch 74/100\n",
      "12/12 [==============================] - 1s 85ms/step - loss: 1.6459e-04 - val_loss: 0.0029\n",
      "Epoch 75/100\n",
      "12/12 [==============================] - 1s 86ms/step - loss: 1.7512e-04 - val_loss: 8.4306e-04\n",
      "Epoch 76/100\n",
      "12/12 [==============================] - 1s 85ms/step - loss: 1.7672e-04 - val_loss: 0.0016\n",
      "Epoch 77/100\n",
      "12/12 [==============================] - 1s 86ms/step - loss: 1.1715e-04 - val_loss: 9.9163e-04\n",
      "Epoch 78/100\n",
      "12/12 [==============================] - 1s 85ms/step - loss: 1.9864e-04 - val_loss: 8.2649e-04\n",
      "Epoch 79/100\n",
      "12/12 [==============================] - 1s 85ms/step - loss: 1.4987e-04 - val_loss: 8.5668e-04\n",
      "Epoch 80/100\n",
      "12/12 [==============================] - 1s 86ms/step - loss: 1.5994e-04 - val_loss: 0.0017\n",
      "Epoch 81/100\n",
      "12/12 [==============================] - 1s 86ms/step - loss: 1.5665e-04 - val_loss: 0.0010\n",
      "Epoch 82/100\n",
      "12/12 [==============================] - 1s 87ms/step - loss: 1.3290e-04 - val_loss: 0.0012\n",
      "Epoch 83/100\n",
      "12/12 [==============================] - 1s 91ms/step - loss: 1.8704e-04 - val_loss: 7.5736e-04\n",
      "Epoch 84/100\n",
      "12/12 [==============================] - 1s 85ms/step - loss: 1.4256e-04 - val_loss: 8.1919e-04\n",
      "Epoch 85/100\n",
      "12/12 [==============================] - 1s 86ms/step - loss: 1.8156e-04 - val_loss: 0.0014\n",
      "Epoch 86/100\n",
      "12/12 [==============================] - 1s 86ms/step - loss: 1.2927e-04 - val_loss: 0.0011\n",
      "Epoch 87/100\n",
      "12/12 [==============================] - 1s 85ms/step - loss: 1.4899e-04 - val_loss: 7.6000e-04\n",
      "Epoch 88/100\n",
      "12/12 [==============================] - 1s 87ms/step - loss: 1.3685e-04 - val_loss: 8.4926e-04\n",
      "Epoch 89/100\n",
      "12/12 [==============================] - 1s 87ms/step - loss: 1.8465e-04 - val_loss: 9.3990e-04\n",
      "Epoch 90/100\n",
      "12/12 [==============================] - 1s 86ms/step - loss: 1.5116e-04 - val_loss: 0.0014\n",
      "Epoch 91/100\n",
      "12/12 [==============================] - 1s 86ms/step - loss: 1.3447e-04 - val_loss: 0.0020\n",
      "Epoch 92/100\n",
      "12/12 [==============================] - 1s 86ms/step - loss: 1.2005e-04 - val_loss: 0.0023\n",
      "Epoch 93/100\n",
      "12/12 [==============================] - 1s 85ms/step - loss: 1.5468e-04 - val_loss: 0.0018\n",
      "Epoch 94/100\n",
      "12/12 [==============================] - 1s 91ms/step - loss: 1.7602e-04 - val_loss: 0.0011\n",
      "Epoch 95/100\n",
      "12/12 [==============================] - 1s 88ms/step - loss: 1.6956e-04 - val_loss: 7.2863e-04\n",
      "Epoch 96/100\n",
      "12/12 [==============================] - 1s 88ms/step - loss: 1.0287e-04 - val_loss: 0.0012\n",
      "Epoch 97/100\n",
      "12/12 [==============================] - 1s 86ms/step - loss: 1.6538e-04 - val_loss: 0.0030\n",
      "Epoch 98/100\n",
      "12/12 [==============================] - 1s 86ms/step - loss: 1.3720e-04 - val_loss: 0.0015\n",
      "Epoch 99/100\n",
      "12/12 [==============================] - 1s 86ms/step - loss: 1.3087e-04 - val_loss: 8.1594e-04\n",
      "Epoch 100/100\n",
      "12/12 [==============================] - 1s 87ms/step - loss: 1.5917e-04 - val_loss: 7.7618e-04\n",
      "23/23 [==============================] - 1s 11ms/step\n",
      "11/11 [==============================] - 0s 12ms/step\n",
      "Model: \"sequential_14\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " bidirectional_4 (Bidirectio  (None, 100)              20800     \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " dense_14 (Dense)            (None, 1)                 101       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 20,901\n",
      "Trainable params: 20,901\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Aviral\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\optimizers\\optimizer_v2\\rmsprop.py:140: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 4s 108ms/step - loss: 0.0711 - val_loss: 0.4303\n",
      "Epoch 2/100\n",
      "12/12 [==============================] - 1s 60ms/step - loss: 0.0512 - val_loss: 0.3479\n",
      "Epoch 3/100\n",
      "12/12 [==============================] - 1s 66ms/step - loss: 0.0381 - val_loss: 0.2747\n",
      "Epoch 4/100\n",
      "12/12 [==============================] - 1s 69ms/step - loss: 0.0272 - val_loss: 0.2094\n",
      "Epoch 5/100\n",
      "12/12 [==============================] - 1s 70ms/step - loss: 0.0185 - val_loss: 0.1527\n",
      "Epoch 6/100\n",
      "12/12 [==============================] - 1s 77ms/step - loss: 0.0119 - val_loss: 0.1033\n",
      "Epoch 7/100\n",
      "12/12 [==============================] - 1s 70ms/step - loss: 0.0078 - val_loss: 0.0666\n",
      "Epoch 8/100\n",
      "12/12 [==============================] - 1s 67ms/step - loss: 0.0058 - val_loss: 0.0462\n",
      "Epoch 9/100\n",
      "12/12 [==============================] - 1s 68ms/step - loss: 0.0046 - val_loss: 0.0323\n",
      "Epoch 10/100\n",
      "12/12 [==============================] - 1s 67ms/step - loss: 0.0035 - val_loss: 0.0224\n",
      "Epoch 11/100\n",
      "12/12 [==============================] - 1s 65ms/step - loss: 0.0025 - val_loss: 0.0129\n",
      "Epoch 12/100\n",
      "12/12 [==============================] - 1s 67ms/step - loss: 0.0018 - val_loss: 0.0073\n",
      "Epoch 13/100\n",
      "12/12 [==============================] - 1s 67ms/step - loss: 0.0013 - val_loss: 0.0058\n",
      "Epoch 14/100\n",
      "12/12 [==============================] - 1s 67ms/step - loss: 9.8256e-04 - val_loss: 0.0052\n",
      "Epoch 15/100\n",
      "12/12 [==============================] - 1s 73ms/step - loss: 8.6896e-04 - val_loss: 0.0049\n",
      "Epoch 16/100\n",
      "12/12 [==============================] - 1s 70ms/step - loss: 8.0309e-04 - val_loss: 0.0061\n",
      "Epoch 17/100\n",
      "12/12 [==============================] - 1s 70ms/step - loss: 7.8209e-04 - val_loss: 0.0045\n",
      "Epoch 18/100\n",
      "12/12 [==============================] - 1s 70ms/step - loss: 7.4129e-04 - val_loss: 0.0045\n",
      "Epoch 19/100\n",
      "12/12 [==============================] - 1s 77ms/step - loss: 7.0236e-04 - val_loss: 0.0054\n",
      "Epoch 20/100\n",
      "12/12 [==============================] - 1s 69ms/step - loss: 6.8083e-04 - val_loss: 0.0038\n",
      "Epoch 21/100\n",
      "12/12 [==============================] - 1s 75ms/step - loss: 6.5154e-04 - val_loss: 0.0035\n",
      "Epoch 22/100\n",
      "12/12 [==============================] - 1s 73ms/step - loss: 6.5032e-04 - val_loss: 0.0034\n",
      "Epoch 23/100\n",
      "12/12 [==============================] - 1s 69ms/step - loss: 6.1020e-04 - val_loss: 0.0033\n",
      "Epoch 24/100\n",
      "12/12 [==============================] - 1s 70ms/step - loss: 6.0097e-04 - val_loss: 0.0032\n",
      "Epoch 25/100\n",
      "12/12 [==============================] - 1s 69ms/step - loss: 5.7582e-04 - val_loss: 0.0031\n",
      "Epoch 26/100\n",
      "12/12 [==============================] - 1s 72ms/step - loss: 5.6804e-04 - val_loss: 0.0033\n",
      "Epoch 27/100\n",
      "12/12 [==============================] - 1s 70ms/step - loss: 5.4743e-04 - val_loss: 0.0032\n",
      "Epoch 28/100\n",
      "12/12 [==============================] - 1s 69ms/step - loss: 5.3612e-04 - val_loss: 0.0042\n",
      "Epoch 29/100\n",
      "12/12 [==============================] - 1s 72ms/step - loss: 5.4781e-04 - val_loss: 0.0037\n",
      "Epoch 30/100\n",
      "12/12 [==============================] - 1s 69ms/step - loss: 5.1772e-04 - val_loss: 0.0029\n",
      "Epoch 31/100\n",
      "12/12 [==============================] - 1s 68ms/step - loss: 5.0957e-04 - val_loss: 0.0028\n",
      "Epoch 32/100\n",
      "12/12 [==============================] - 1s 68ms/step - loss: 5.0603e-04 - val_loss: 0.0027\n",
      "Epoch 33/100\n",
      "12/12 [==============================] - 1s 68ms/step - loss: 4.9541e-04 - val_loss: 0.0026\n",
      "Epoch 34/100\n",
      "12/12 [==============================] - 1s 71ms/step - loss: 4.7419e-04 - val_loss: 0.0025\n",
      "Epoch 35/100\n",
      "12/12 [==============================] - 1s 70ms/step - loss: 4.6713e-04 - val_loss: 0.0032\n",
      "Epoch 36/100\n",
      "12/12 [==============================] - 1s 69ms/step - loss: 4.7525e-04 - val_loss: 0.0024\n",
      "Epoch 37/100\n",
      "12/12 [==============================] - 1s 71ms/step - loss: 4.6072e-04 - val_loss: 0.0025\n",
      "Epoch 38/100\n",
      "12/12 [==============================] - 1s 68ms/step - loss: 4.5703e-04 - val_loss: 0.0026\n",
      "Epoch 39/100\n",
      "12/12 [==============================] - 1s 71ms/step - loss: 4.5347e-04 - val_loss: 0.0029\n",
      "Epoch 40/100\n",
      "12/12 [==============================] - 1s 73ms/step - loss: 4.4707e-04 - val_loss: 0.0023\n",
      "Epoch 41/100\n",
      "12/12 [==============================] - 1s 68ms/step - loss: 4.4078e-04 - val_loss: 0.0024\n",
      "Epoch 42/100\n",
      "12/12 [==============================] - 1s 70ms/step - loss: 4.4372e-04 - val_loss: 0.0028\n",
      "Epoch 43/100\n",
      "12/12 [==============================] - 1s 71ms/step - loss: 4.3960e-04 - val_loss: 0.0038\n",
      "Epoch 44/100\n",
      "12/12 [==============================] - 1s 71ms/step - loss: 4.5089e-04 - val_loss: 0.0028\n",
      "Epoch 45/100\n",
      "12/12 [==============================] - 1s 73ms/step - loss: 4.2333e-04 - val_loss: 0.0025\n",
      "Epoch 46/100\n",
      "12/12 [==============================] - 1s 69ms/step - loss: 4.1300e-04 - val_loss: 0.0022\n",
      "Epoch 47/100\n",
      "12/12 [==============================] - 1s 68ms/step - loss: 4.1658e-04 - val_loss: 0.0021\n",
      "Epoch 48/100\n",
      "12/12 [==============================] - 1s 69ms/step - loss: 4.1008e-04 - val_loss: 0.0021\n",
      "Epoch 49/100\n",
      "12/12 [==============================] - 1s 67ms/step - loss: 4.0223e-04 - val_loss: 0.0021\n",
      "Epoch 50/100\n",
      "12/12 [==============================] - 1s 67ms/step - loss: 4.0856e-04 - val_loss: 0.0020\n",
      "Epoch 51/100\n",
      "12/12 [==============================] - 1s 67ms/step - loss: 3.9753e-04 - val_loss: 0.0021\n",
      "Epoch 52/100\n",
      "12/12 [==============================] - 1s 68ms/step - loss: 3.8760e-04 - val_loss: 0.0021\n",
      "Epoch 53/100\n",
      "12/12 [==============================] - 1s 67ms/step - loss: 3.9084e-04 - val_loss: 0.0023\n",
      "Epoch 54/100\n",
      "12/12 [==============================] - 1s 70ms/step - loss: 3.8555e-04 - val_loss: 0.0021\n",
      "Epoch 55/100\n",
      "12/12 [==============================] - 1s 70ms/step - loss: 3.8669e-04 - val_loss: 0.0026\n",
      "Epoch 56/100\n",
      "12/12 [==============================] - 1s 68ms/step - loss: 3.8325e-04 - val_loss: 0.0020\n",
      "Epoch 57/100\n",
      "12/12 [==============================] - 1s 70ms/step - loss: 3.7848e-04 - val_loss: 0.0019\n",
      "Epoch 58/100\n",
      "12/12 [==============================] - 1s 74ms/step - loss: 3.7160e-04 - val_loss: 0.0019\n",
      "Epoch 59/100\n",
      "12/12 [==============================] - 1s 73ms/step - loss: 3.7059e-04 - val_loss: 0.0020\n",
      "Epoch 60/100\n",
      "12/12 [==============================] - 1s 71ms/step - loss: 3.7159e-04 - val_loss: 0.0025\n",
      "Epoch 61/100\n",
      "12/12 [==============================] - 1s 74ms/step - loss: 3.7511e-04 - val_loss: 0.0018\n",
      "Epoch 62/100\n",
      "12/12 [==============================] - 1s 74ms/step - loss: 3.6298e-04 - val_loss: 0.0024\n",
      "Epoch 63/100\n",
      "12/12 [==============================] - 1s 67ms/step - loss: 3.5692e-04 - val_loss: 0.0025\n",
      "Epoch 64/100\n",
      "12/12 [==============================] - 1s 68ms/step - loss: 3.5868e-04 - val_loss: 0.0018\n",
      "Epoch 65/100\n",
      "12/12 [==============================] - 1s 68ms/step - loss: 3.6193e-04 - val_loss: 0.0019\n",
      "Epoch 66/100\n",
      "12/12 [==============================] - 1s 69ms/step - loss: 3.4681e-04 - val_loss: 0.0017\n",
      "Epoch 67/100\n",
      "12/12 [==============================] - 1s 72ms/step - loss: 3.4412e-04 - val_loss: 0.0021\n",
      "Epoch 68/100\n",
      "12/12 [==============================] - 1s 76ms/step - loss: 3.4294e-04 - val_loss: 0.0019\n",
      "Epoch 69/100\n",
      "12/12 [==============================] - 1s 73ms/step - loss: 3.4861e-04 - val_loss: 0.0021\n",
      "Epoch 70/100\n",
      "12/12 [==============================] - 1s 72ms/step - loss: 3.3854e-04 - val_loss: 0.0020\n",
      "Epoch 71/100\n",
      "12/12 [==============================] - 1s 70ms/step - loss: 3.3377e-04 - val_loss: 0.0016\n",
      "Epoch 72/100\n",
      "12/12 [==============================] - 1s 68ms/step - loss: 3.3540e-04 - val_loss: 0.0016\n",
      "Epoch 73/100\n",
      "12/12 [==============================] - 1s 67ms/step - loss: 3.3888e-04 - val_loss: 0.0017\n",
      "Epoch 74/100\n",
      "12/12 [==============================] - 1s 69ms/step - loss: 3.2866e-04 - val_loss: 0.0024\n",
      "Epoch 75/100\n",
      "12/12 [==============================] - 1s 69ms/step - loss: 3.3656e-04 - val_loss: 0.0015\n",
      "Epoch 76/100\n",
      "12/12 [==============================] - 1s 67ms/step - loss: 3.1884e-04 - val_loss: 0.0020\n",
      "Epoch 77/100\n",
      "12/12 [==============================] - 1s 67ms/step - loss: 3.2818e-04 - val_loss: 0.0015\n",
      "Epoch 78/100\n",
      "12/12 [==============================] - 1s 74ms/step - loss: 3.2751e-04 - val_loss: 0.0015\n",
      "Epoch 79/100\n",
      "12/12 [==============================] - 1s 70ms/step - loss: 3.1789e-04 - val_loss: 0.0021\n",
      "Epoch 80/100\n",
      "12/12 [==============================] - 1s 68ms/step - loss: 3.1311e-04 - val_loss: 0.0016\n",
      "Epoch 81/100\n",
      "12/12 [==============================] - 1s 67ms/step - loss: 3.2184e-04 - val_loss: 0.0016\n",
      "Epoch 82/100\n",
      "12/12 [==============================] - 1s 67ms/step - loss: 3.1784e-04 - val_loss: 0.0014\n",
      "Epoch 83/100\n",
      "12/12 [==============================] - 1s 68ms/step - loss: 3.0639e-04 - val_loss: 0.0023\n",
      "Epoch 84/100\n",
      "12/12 [==============================] - 1s 68ms/step - loss: 3.1071e-04 - val_loss: 0.0014\n",
      "Epoch 85/100\n",
      "12/12 [==============================] - 1s 76ms/step - loss: 3.0376e-04 - val_loss: 0.0016\n",
      "Epoch 86/100\n",
      "12/12 [==============================] - 1s 68ms/step - loss: 3.1049e-04 - val_loss: 0.0014\n",
      "Epoch 87/100\n",
      "12/12 [==============================] - 1s 67ms/step - loss: 3.0648e-04 - val_loss: 0.0013\n",
      "Epoch 88/100\n",
      "12/12 [==============================] - 1s 68ms/step - loss: 3.0113e-04 - val_loss: 0.0013\n",
      "Epoch 89/100\n",
      "12/12 [==============================] - 1s 68ms/step - loss: 2.9340e-04 - val_loss: 0.0013\n",
      "Epoch 90/100\n",
      "12/12 [==============================] - 1s 69ms/step - loss: 3.1082e-04 - val_loss: 0.0013\n",
      "Epoch 91/100\n",
      "12/12 [==============================] - 1s 67ms/step - loss: 2.9716e-04 - val_loss: 0.0020\n",
      "Epoch 92/100\n",
      "12/12 [==============================] - 1s 68ms/step - loss: 2.9011e-04 - val_loss: 0.0013\n",
      "Epoch 93/100\n",
      "12/12 [==============================] - 1s 67ms/step - loss: 3.0117e-04 - val_loss: 0.0018\n",
      "Epoch 94/100\n",
      "12/12 [==============================] - 1s 67ms/step - loss: 2.8599e-04 - val_loss: 0.0013\n",
      "Epoch 95/100\n",
      "12/12 [==============================] - 1s 67ms/step - loss: 2.9382e-04 - val_loss: 0.0014\n",
      "Epoch 96/100\n",
      "12/12 [==============================] - 1s 69ms/step - loss: 2.8893e-04 - val_loss: 0.0019\n",
      "Epoch 97/100\n",
      "12/12 [==============================] - 1s 70ms/step - loss: 2.8848e-04 - val_loss: 0.0014\n",
      "Epoch 98/100\n",
      "12/12 [==============================] - 1s 68ms/step - loss: 2.8149e-04 - val_loss: 0.0020\n",
      "Epoch 99/100\n",
      "12/12 [==============================] - 1s 68ms/step - loss: 2.8533e-04 - val_loss: 0.0013\n",
      "Epoch 100/100\n",
      "12/12 [==============================] - 1s 68ms/step - loss: 2.8406e-04 - val_loss: 0.0016\n",
      "23/23 [==============================] - 1s 10ms/step\n",
      "11/11 [==============================] - 0s 12ms/step\n",
      "Model: \"sequential_15\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_17 (LSTM)              (None, 100, 50)           10400     \n",
      "                                                                 \n",
      " lstm_18 (LSTM)              (None, 50)                20200     \n",
      "                                                                 \n",
      " dense_15 (Dense)            (None, 1)                 51        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 30,651\n",
      "Trainable params: 30,651\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Aviral\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\optimizers\\optimizer_v2\\rmsprop.py:140: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 4s 141ms/step - loss: 0.4973 - val_loss: 0.0649\n",
      "Epoch 2/100\n",
      "12/12 [==============================] - 1s 102ms/step - loss: 0.0073 - val_loss: 0.0383\n",
      "Epoch 3/100\n",
      "12/12 [==============================] - 1s 100ms/step - loss: 0.0054 - val_loss: 0.0317\n",
      "Epoch 4/100\n",
      "12/12 [==============================] - 1s 97ms/step - loss: 0.0087 - val_loss: 0.0038\n",
      "Epoch 5/100\n",
      "12/12 [==============================] - 1s 99ms/step - loss: 0.0055 - val_loss: 0.0061\n",
      "Epoch 6/100\n",
      "12/12 [==============================] - 1s 100ms/step - loss: 0.0063 - val_loss: 0.0037\n",
      "Epoch 7/100\n",
      "12/12 [==============================] - 1s 99ms/step - loss: 0.0053 - val_loss: 0.0040\n",
      "Epoch 8/100\n",
      "12/12 [==============================] - 1s 97ms/step - loss: 0.0097 - val_loss: 0.0062\n",
      "Epoch 9/100\n",
      "12/12 [==============================] - 1s 95ms/step - loss: 0.0027 - val_loss: 0.0308\n",
      "Epoch 10/100\n",
      "12/12 [==============================] - 1s 98ms/step - loss: 0.0042 - val_loss: 0.0847\n",
      "Epoch 11/100\n",
      "12/12 [==============================] - 1s 95ms/step - loss: 0.0049 - val_loss: 0.0286\n",
      "Epoch 12/100\n",
      "12/12 [==============================] - 1s 95ms/step - loss: 0.0030 - val_loss: 0.0498\n",
      "Epoch 13/100\n",
      "12/12 [==============================] - 1s 101ms/step - loss: 0.0057 - val_loss: 0.0196\n",
      "Epoch 14/100\n",
      "12/12 [==============================] - 1s 98ms/step - loss: 0.0016 - val_loss: 0.0444\n",
      "Epoch 15/100\n",
      "12/12 [==============================] - 1s 108ms/step - loss: 0.0047 - val_loss: 0.0747\n",
      "Epoch 16/100\n",
      "12/12 [==============================] - 1s 99ms/step - loss: 0.0021 - val_loss: 0.0226\n",
      "Epoch 17/100\n",
      "12/12 [==============================] - 1s 95ms/step - loss: 0.0018 - val_loss: 0.0134\n",
      "Epoch 18/100\n",
      "12/12 [==============================] - 1s 97ms/step - loss: 0.0013 - val_loss: 0.0076\n",
      "Epoch 19/100\n",
      "12/12 [==============================] - 1s 96ms/step - loss: 0.0027 - val_loss: 0.0212\n",
      "Epoch 20/100\n",
      "12/12 [==============================] - 1s 111ms/step - loss: 0.0011 - val_loss: 0.0320\n",
      "Epoch 21/100\n",
      "12/12 [==============================] - 1s 100ms/step - loss: 0.0022 - val_loss: 0.0154\n",
      "Epoch 22/100\n",
      "12/12 [==============================] - 1s 99ms/step - loss: 0.0010 - val_loss: 0.0276\n",
      "Epoch 23/100\n",
      "12/12 [==============================] - 1s 102ms/step - loss: 0.0025 - val_loss: 0.0261\n",
      "Epoch 24/100\n",
      "12/12 [==============================] - 1s 103ms/step - loss: 7.4823e-04 - val_loss: 0.0168\n",
      "Epoch 25/100\n",
      "12/12 [==============================] - 1s 99ms/step - loss: 9.8995e-04 - val_loss: 0.0421\n",
      "Epoch 26/100\n",
      "12/12 [==============================] - 1s 98ms/step - loss: 0.0021 - val_loss: 0.0085\n",
      "Epoch 27/100\n",
      "12/12 [==============================] - 1s 99ms/step - loss: 3.7019e-04 - val_loss: 0.0093\n",
      "Epoch 28/100\n",
      "12/12 [==============================] - 1s 99ms/step - loss: 0.0013 - val_loss: 0.0049\n",
      "Epoch 29/100\n",
      "12/12 [==============================] - 1s 99ms/step - loss: 0.0011 - val_loss: 0.0102\n",
      "Epoch 30/100\n",
      "12/12 [==============================] - 1s 98ms/step - loss: 0.0016 - val_loss: 0.0342\n",
      "Epoch 31/100\n",
      "12/12 [==============================] - 1s 107ms/step - loss: 3.9858e-04 - val_loss: 0.0036\n",
      "Epoch 32/100\n",
      "12/12 [==============================] - 2s 142ms/step - loss: 0.0011 - val_loss: 0.0155\n",
      "Epoch 33/100\n",
      "12/12 [==============================] - 1s 106ms/step - loss: 0.0012 - val_loss: 0.0145\n",
      "Epoch 34/100\n",
      "12/12 [==============================] - 1s 99ms/step - loss: 8.1571e-04 - val_loss: 0.0058\n",
      "Epoch 35/100\n",
      "12/12 [==============================] - 1s 101ms/step - loss: 9.3474e-04 - val_loss: 0.0058\n",
      "Epoch 36/100\n",
      "12/12 [==============================] - 1s 109ms/step - loss: 0.0011 - val_loss: 0.0507\n",
      "Epoch 37/100\n",
      "12/12 [==============================] - 1s 113ms/step - loss: 5.6229e-04 - val_loss: 0.0229\n",
      "Epoch 38/100\n",
      "12/12 [==============================] - 1s 103ms/step - loss: 7.9175e-04 - val_loss: 0.0118\n",
      "Epoch 39/100\n",
      "12/12 [==============================] - 1s 101ms/step - loss: 0.0010 - val_loss: 0.0110\n",
      "Epoch 40/100\n",
      "12/12 [==============================] - 1s 102ms/step - loss: 6.0819e-04 - val_loss: 0.0136\n",
      "Epoch 41/100\n",
      "12/12 [==============================] - 1s 99ms/step - loss: 0.0012 - val_loss: 0.0058\n",
      "Epoch 42/100\n",
      "12/12 [==============================] - 1s 103ms/step - loss: 5.5320e-04 - val_loss: 0.0152\n",
      "Epoch 43/100\n",
      "12/12 [==============================] - 1s 102ms/step - loss: 5.9708e-04 - val_loss: 0.0185\n",
      "Epoch 44/100\n",
      "12/12 [==============================] - 1s 100ms/step - loss: 8.6727e-04 - val_loss: 0.0153\n",
      "Epoch 45/100\n",
      "12/12 [==============================] - 1s 100ms/step - loss: 4.4737e-04 - val_loss: 0.0182\n",
      "Epoch 46/100\n",
      "12/12 [==============================] - 1s 98ms/step - loss: 9.5297e-04 - val_loss: 0.0019\n",
      "Epoch 47/100\n",
      "12/12 [==============================] - 1s 99ms/step - loss: 4.3340e-04 - val_loss: 0.0061\n",
      "Epoch 48/100\n",
      "12/12 [==============================] - 1s 99ms/step - loss: 9.6438e-04 - val_loss: 0.0019\n",
      "Epoch 49/100\n",
      "12/12 [==============================] - 1s 100ms/step - loss: 4.4180e-04 - val_loss: 0.0173\n",
      "Epoch 50/100\n",
      "12/12 [==============================] - 1s 100ms/step - loss: 5.9586e-04 - val_loss: 0.0035\n",
      "Epoch 51/100\n",
      "12/12 [==============================] - 1s 99ms/step - loss: 5.3806e-04 - val_loss: 0.0034\n",
      "Epoch 52/100\n",
      "12/12 [==============================] - 1s 100ms/step - loss: 5.4805e-04 - val_loss: 0.0074\n",
      "Epoch 53/100\n",
      "12/12 [==============================] - 1s 99ms/step - loss: 7.3052e-04 - val_loss: 0.0031\n",
      "Epoch 54/100\n",
      "12/12 [==============================] - 1s 97ms/step - loss: 6.6640e-04 - val_loss: 0.0032\n",
      "Epoch 55/100\n",
      "12/12 [==============================] - 1s 97ms/step - loss: 4.6787e-04 - val_loss: 0.0042\n",
      "Epoch 56/100\n",
      "12/12 [==============================] - 1s 96ms/step - loss: 5.4288e-04 - val_loss: 0.0021\n",
      "Epoch 57/100\n",
      "12/12 [==============================] - 1s 97ms/step - loss: 5.8876e-04 - val_loss: 0.0060\n",
      "Epoch 58/100\n",
      "12/12 [==============================] - 1s 98ms/step - loss: 3.6358e-04 - val_loss: 0.0192\n",
      "Epoch 59/100\n",
      "12/12 [==============================] - 1s 95ms/step - loss: 7.0448e-04 - val_loss: 0.0073\n",
      "Epoch 60/100\n",
      "12/12 [==============================] - 1s 97ms/step - loss: 4.3220e-04 - val_loss: 0.0016\n",
      "Epoch 61/100\n",
      "12/12 [==============================] - 1s 97ms/step - loss: 8.0574e-04 - val_loss: 0.0015\n",
      "Epoch 62/100\n",
      "12/12 [==============================] - 1s 95ms/step - loss: 2.4952e-04 - val_loss: 0.0032\n",
      "Epoch 63/100\n",
      "12/12 [==============================] - 1s 95ms/step - loss: 5.5237e-04 - val_loss: 0.0118\n",
      "Epoch 64/100\n",
      "12/12 [==============================] - 1s 97ms/step - loss: 6.9866e-04 - val_loss: 0.0062\n",
      "Epoch 65/100\n",
      "12/12 [==============================] - 1s 97ms/step - loss: 2.7828e-04 - val_loss: 0.0017\n",
      "Epoch 66/100\n",
      "12/12 [==============================] - 1s 96ms/step - loss: 4.9806e-04 - val_loss: 0.0124\n",
      "Epoch 67/100\n",
      "12/12 [==============================] - 1s 103ms/step - loss: 6.3679e-04 - val_loss: 0.0032\n",
      "Epoch 68/100\n",
      "12/12 [==============================] - 1s 97ms/step - loss: 3.1030e-04 - val_loss: 0.0022\n",
      "Epoch 69/100\n",
      "12/12 [==============================] - 1s 95ms/step - loss: 5.8656e-04 - val_loss: 0.0060\n",
      "Epoch 70/100\n",
      "12/12 [==============================] - 1s 96ms/step - loss: 4.2446e-04 - val_loss: 0.0028\n",
      "Epoch 71/100\n",
      "12/12 [==============================] - 1s 97ms/step - loss: 3.3300e-04 - val_loss: 0.0024\n",
      "Epoch 72/100\n",
      "12/12 [==============================] - 1s 95ms/step - loss: 5.4255e-04 - val_loss: 0.0015\n",
      "Epoch 73/100\n",
      "12/12 [==============================] - 1s 101ms/step - loss: 5.0084e-04 - val_loss: 0.0099\n",
      "Epoch 74/100\n",
      "12/12 [==============================] - 1s 100ms/step - loss: 4.7673e-04 - val_loss: 0.0035\n",
      "Epoch 75/100\n",
      "12/12 [==============================] - 1s 97ms/step - loss: 2.6422e-04 - val_loss: 0.0052\n",
      "Epoch 76/100\n",
      "12/12 [==============================] - 1s 95ms/step - loss: 5.5447e-04 - val_loss: 0.0021\n",
      "Epoch 77/100\n",
      "12/12 [==============================] - 1s 95ms/step - loss: 3.9778e-04 - val_loss: 0.0015\n",
      "Epoch 78/100\n",
      "12/12 [==============================] - 1s 95ms/step - loss: 4.8762e-04 - val_loss: 0.0049\n",
      "Epoch 79/100\n",
      "12/12 [==============================] - 1s 95ms/step - loss: 3.8776e-04 - val_loss: 0.0038\n",
      "Epoch 80/100\n",
      "12/12 [==============================] - 1s 96ms/step - loss: 4.2635e-04 - val_loss: 0.0103\n",
      "Epoch 81/100\n",
      "12/12 [==============================] - 1s 95ms/step - loss: 3.7818e-04 - val_loss: 0.0108\n",
      "Epoch 82/100\n",
      "12/12 [==============================] - 1s 97ms/step - loss: 4.9649e-04 - val_loss: 0.0041\n",
      "Epoch 83/100\n",
      "12/12 [==============================] - 1s 96ms/step - loss: 2.9992e-04 - val_loss: 0.0011\n",
      "Epoch 84/100\n",
      "12/12 [==============================] - 1s 95ms/step - loss: 3.7124e-04 - val_loss: 0.0089\n",
      "Epoch 85/100\n",
      "12/12 [==============================] - 1s 97ms/step - loss: 3.3058e-04 - val_loss: 0.0240\n",
      "Epoch 86/100\n",
      "12/12 [==============================] - 1s 94ms/step - loss: 3.3972e-04 - val_loss: 0.0175\n",
      "Epoch 87/100\n",
      "12/12 [==============================] - 1s 97ms/step - loss: 4.6702e-04 - val_loss: 0.0035\n",
      "Epoch 88/100\n",
      "12/12 [==============================] - 1s 94ms/step - loss: 3.8660e-04 - val_loss: 0.0044\n",
      "Epoch 89/100\n",
      "12/12 [==============================] - 1s 96ms/step - loss: 4.0230e-04 - val_loss: 0.0118\n",
      "Epoch 90/100\n",
      "12/12 [==============================] - 1s 96ms/step - loss: 3.9459e-04 - val_loss: 0.0126\n",
      "Epoch 91/100\n",
      "12/12 [==============================] - 1s 95ms/step - loss: 4.2369e-04 - val_loss: 0.0018\n",
      "Epoch 92/100\n",
      "12/12 [==============================] - 1s 96ms/step - loss: 3.9188e-04 - val_loss: 0.0099\n",
      "Epoch 93/100\n",
      "12/12 [==============================] - 1s 97ms/step - loss: 4.7540e-04 - val_loss: 0.0060\n",
      "Epoch 94/100\n",
      "12/12 [==============================] - 1s 97ms/step - loss: 2.3317e-04 - val_loss: 0.0245\n",
      "Epoch 95/100\n",
      "12/12 [==============================] - 1s 95ms/step - loss: 4.3296e-04 - val_loss: 0.0025\n",
      "Epoch 96/100\n",
      "12/12 [==============================] - 1s 97ms/step - loss: 2.9901e-04 - val_loss: 0.0014\n",
      "Epoch 97/100\n",
      "12/12 [==============================] - 1s 100ms/step - loss: 4.3572e-04 - val_loss: 0.0020\n",
      "Epoch 98/100\n",
      "12/12 [==============================] - 1s 101ms/step - loss: 4.0705e-04 - val_loss: 0.0032\n",
      "Epoch 99/100\n",
      "12/12 [==============================] - 1s 99ms/step - loss: 1.4726e-04 - val_loss: 0.0154\n",
      "Epoch 100/100\n",
      "12/12 [==============================] - 1s 98ms/step - loss: 5.6283e-04 - val_loss: 0.0026\n",
      "23/23 [==============================] - 1s 16ms/step\n",
      "11/11 [==============================] - 0s 17ms/step\n",
      "Model: \"sequential_16\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_19 (LSTM)              (None, 100, 50)           10400     \n",
      "                                                                 \n",
      " lstm_20 (LSTM)              (None, 50)                20200     \n",
      "                                                                 \n",
      " dense_16 (Dense)            (None, 1)                 51        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 30,651\n",
      "Trainable params: 30,651\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Aviral\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\optimizers\\optimizer_v2\\rmsprop.py:140: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 5s 188ms/step - loss: 0.0064 - val_loss: 0.0053\n",
      "Epoch 2/100\n",
      "12/12 [==============================] - 2s 133ms/step - loss: 9.1200e-04 - val_loss: 0.0037\n",
      "Epoch 3/100\n",
      "12/12 [==============================] - 2s 144ms/step - loss: 0.0015 - val_loss: 0.0053\n",
      "Epoch 4/100\n",
      "12/12 [==============================] - 2s 155ms/step - loss: 9.6176e-04 - val_loss: 0.0032\n",
      "Epoch 5/100\n",
      "12/12 [==============================] - 2s 154ms/step - loss: 0.0011 - val_loss: 0.0027\n",
      "Epoch 6/100\n",
      "12/12 [==============================] - 2s 154ms/step - loss: 7.1975e-04 - val_loss: 0.0029\n",
      "Epoch 7/100\n",
      "12/12 [==============================] - 2s 153ms/step - loss: 9.6116e-04 - val_loss: 0.0104\n",
      "Epoch 8/100\n",
      "12/12 [==============================] - 2s 152ms/step - loss: 6.2497e-04 - val_loss: 0.0055\n",
      "Epoch 9/100\n",
      "12/12 [==============================] - 2s 152ms/step - loss: 9.3755e-04 - val_loss: 0.0069\n",
      "Epoch 10/100\n",
      "12/12 [==============================] - 2s 153ms/step - loss: 7.3942e-04 - val_loss: 0.0147\n",
      "Epoch 11/100\n",
      "12/12 [==============================] - 2s 154ms/step - loss: 8.3221e-04 - val_loss: 0.0050\n",
      "Epoch 12/100\n",
      "12/12 [==============================] - 2s 152ms/step - loss: 5.7168e-04 - val_loss: 0.0048\n",
      "Epoch 13/100\n",
      "12/12 [==============================] - 2s 152ms/step - loss: 8.3572e-04 - val_loss: 0.0025\n",
      "Epoch 14/100\n",
      "12/12 [==============================] - 2s 152ms/step - loss: 4.9735e-04 - val_loss: 0.0058\n",
      "Epoch 15/100\n",
      "12/12 [==============================] - 2s 152ms/step - loss: 5.8551e-04 - val_loss: 0.0288\n",
      "Epoch 16/100\n",
      "12/12 [==============================] - 2s 151ms/step - loss: 7.3540e-04 - val_loss: 0.0012\n",
      "Epoch 17/100\n",
      "12/12 [==============================] - 2s 152ms/step - loss: 6.6379e-04 - val_loss: 0.0031\n",
      "Epoch 18/100\n",
      "12/12 [==============================] - 2s 153ms/step - loss: 4.2109e-04 - val_loss: 0.0013\n",
      "Epoch 19/100\n",
      "12/12 [==============================] - 2s 153ms/step - loss: 6.4942e-04 - val_loss: 0.0019\n",
      "Epoch 20/100\n",
      "12/12 [==============================] - 2s 155ms/step - loss: 5.6710e-04 - val_loss: 0.0028\n",
      "Epoch 21/100\n",
      "12/12 [==============================] - 2s 153ms/step - loss: 4.1000e-04 - val_loss: 0.0025\n",
      "Epoch 22/100\n",
      "12/12 [==============================] - 2s 153ms/step - loss: 5.5547e-04 - val_loss: 9.9826e-04\n",
      "Epoch 23/100\n",
      "12/12 [==============================] - 2s 152ms/step - loss: 5.1996e-04 - val_loss: 0.0030\n",
      "Epoch 24/100\n",
      "12/12 [==============================] - 2s 151ms/step - loss: 4.6168e-04 - val_loss: 0.0010\n",
      "Epoch 25/100\n",
      "12/12 [==============================] - 2s 152ms/step - loss: 2.7134e-04 - val_loss: 0.0029\n",
      "Epoch 26/100\n",
      "12/12 [==============================] - 2s 151ms/step - loss: 4.1072e-04 - val_loss: 0.0014\n",
      "Epoch 27/100\n",
      "12/12 [==============================] - 2s 153ms/step - loss: 5.0084e-04 - val_loss: 0.0012\n",
      "Epoch 28/100\n",
      "12/12 [==============================] - 2s 154ms/step - loss: 4.4872e-04 - val_loss: 0.0012\n",
      "Epoch 29/100\n",
      "12/12 [==============================] - 2s 152ms/step - loss: 4.1739e-04 - val_loss: 0.0014\n",
      "Epoch 30/100\n",
      "12/12 [==============================] - 2s 152ms/step - loss: 2.8969e-04 - val_loss: 0.0014\n",
      "Epoch 31/100\n",
      "12/12 [==============================] - 2s 152ms/step - loss: 4.6727e-04 - val_loss: 0.0011\n",
      "Epoch 32/100\n",
      "12/12 [==============================] - 2s 153ms/step - loss: 4.2515e-04 - val_loss: 0.0059\n",
      "Epoch 33/100\n",
      "12/12 [==============================] - 2s 159ms/step - loss: 2.4445e-04 - val_loss: 0.0040\n",
      "Epoch 34/100\n",
      "12/12 [==============================] - 2s 152ms/step - loss: 3.3925e-04 - val_loss: 0.0028\n",
      "Epoch 35/100\n",
      "12/12 [==============================] - 2s 152ms/step - loss: 4.2273e-04 - val_loss: 0.0011\n",
      "Epoch 36/100\n",
      "12/12 [==============================] - 2s 154ms/step - loss: 4.0297e-04 - val_loss: 0.0015\n",
      "Epoch 37/100\n",
      "12/12 [==============================] - 2s 154ms/step - loss: 3.2609e-04 - val_loss: 0.0057\n",
      "Epoch 38/100\n",
      "12/12 [==============================] - 2s 160ms/step - loss: 2.7246e-04 - val_loss: 0.0081\n",
      "Epoch 39/100\n",
      "12/12 [==============================] - 2s 160ms/step - loss: 4.1160e-04 - val_loss: 0.0022\n",
      "Epoch 40/100\n",
      "12/12 [==============================] - 2s 156ms/step - loss: 3.3178e-04 - val_loss: 0.0046\n",
      "Epoch 41/100\n",
      "12/12 [==============================] - 2s 153ms/step - loss: 2.3778e-04 - val_loss: 0.0022\n",
      "Epoch 42/100\n",
      "12/12 [==============================] - 2s 153ms/step - loss: 3.7035e-04 - val_loss: 0.0080\n",
      "Epoch 43/100\n",
      "12/12 [==============================] - 2s 153ms/step - loss: 3.2989e-04 - val_loss: 0.0035\n",
      "Epoch 44/100\n",
      "12/12 [==============================] - 2s 153ms/step - loss: 3.4573e-04 - val_loss: 8.5075e-04\n",
      "Epoch 45/100\n",
      "12/12 [==============================] - 2s 154ms/step - loss: 3.1085e-04 - val_loss: 7.1300e-04\n",
      "Epoch 46/100\n",
      "12/12 [==============================] - 2s 150ms/step - loss: 3.9819e-04 - val_loss: 7.9579e-04\n",
      "Epoch 47/100\n",
      "12/12 [==============================] - 2s 153ms/step - loss: 2.2982e-04 - val_loss: 0.0046\n",
      "Epoch 48/100\n",
      "12/12 [==============================] - 2s 152ms/step - loss: 3.3604e-04 - val_loss: 8.3710e-04\n",
      "Epoch 49/100\n",
      "12/12 [==============================] - 2s 152ms/step - loss: 3.6801e-04 - val_loss: 0.0012\n",
      "Epoch 50/100\n",
      "12/12 [==============================] - 2s 152ms/step - loss: 1.1829e-04 - val_loss: 0.0018\n",
      "Epoch 51/100\n",
      "12/12 [==============================] - 2s 152ms/step - loss: 4.0730e-04 - val_loss: 7.9785e-04\n",
      "Epoch 52/100\n",
      "12/12 [==============================] - 2s 157ms/step - loss: 3.6110e-04 - val_loss: 7.9187e-04\n",
      "Epoch 53/100\n",
      "12/12 [==============================] - 2s 205ms/step - loss: 1.1183e-04 - val_loss: 0.0048\n",
      "Epoch 54/100\n",
      "12/12 [==============================] - 2s 156ms/step - loss: 3.5951e-04 - val_loss: 8.5930e-04\n",
      "Epoch 55/100\n",
      "12/12 [==============================] - 2s 156ms/step - loss: 2.9330e-04 - val_loss: 7.3454e-04\n",
      "Epoch 56/100\n",
      "12/12 [==============================] - 2s 159ms/step - loss: 1.4744e-04 - val_loss: 0.0011\n",
      "Epoch 57/100\n",
      "12/12 [==============================] - 2s 158ms/step - loss: 3.3443e-04 - val_loss: 8.6509e-04\n",
      "Epoch 58/100\n",
      "12/12 [==============================] - 2s 156ms/step - loss: 1.6950e-04 - val_loss: 0.0083\n",
      "Epoch 59/100\n",
      "12/12 [==============================] - 2s 155ms/step - loss: 2.2141e-04 - val_loss: 6.8101e-04\n",
      "Epoch 60/100\n",
      "12/12 [==============================] - 2s 164ms/step - loss: 3.6104e-04 - val_loss: 7.5545e-04\n",
      "Epoch 61/100\n",
      "12/12 [==============================] - 2s 157ms/step - loss: 2.3064e-04 - val_loss: 9.3886e-04\n",
      "Epoch 62/100\n",
      "12/12 [==============================] - 2s 155ms/step - loss: 2.6057e-04 - val_loss: 9.5336e-04\n",
      "Epoch 63/100\n",
      "12/12 [==============================] - 2s 156ms/step - loss: 2.0709e-04 - val_loss: 0.0017\n",
      "Epoch 64/100\n",
      "12/12 [==============================] - 2s 154ms/step - loss: 3.0204e-04 - val_loss: 7.9173e-04\n",
      "Epoch 65/100\n",
      "12/12 [==============================] - 2s 156ms/step - loss: 2.1309e-04 - val_loss: 0.0025\n",
      "Epoch 66/100\n",
      "12/12 [==============================] - 2s 155ms/step - loss: 3.2298e-04 - val_loss: 6.4069e-04\n",
      "Epoch 67/100\n",
      "12/12 [==============================] - 2s 155ms/step - loss: 1.4870e-04 - val_loss: 0.0053\n",
      "Epoch 68/100\n",
      "12/12 [==============================] - 2s 158ms/step - loss: 2.6212e-04 - val_loss: 0.0027\n",
      "Epoch 69/100\n",
      "12/12 [==============================] - 2s 155ms/step - loss: 2.1060e-04 - val_loss: 0.0041\n",
      "Epoch 70/100\n",
      "12/12 [==============================] - 2s 154ms/step - loss: 2.9154e-04 - val_loss: 9.8518e-04\n",
      "Epoch 71/100\n",
      "12/12 [==============================] - 2s 162ms/step - loss: 2.0723e-04 - val_loss: 0.0029\n",
      "Epoch 72/100\n",
      "12/12 [==============================] - 2s 151ms/step - loss: 2.5714e-04 - val_loss: 6.0532e-04\n",
      "Epoch 73/100\n",
      "12/12 [==============================] - 2s 156ms/step - loss: 2.3555e-04 - val_loss: 6.3991e-04\n",
      "Epoch 74/100\n",
      "12/12 [==============================] - 2s 154ms/step - loss: 2.2798e-04 - val_loss: 6.0614e-04\n",
      "Epoch 75/100\n",
      "12/12 [==============================] - 2s 153ms/step - loss: 2.7224e-04 - val_loss: 6.1409e-04\n",
      "Epoch 76/100\n",
      "12/12 [==============================] - 2s 161ms/step - loss: 2.0560e-04 - val_loss: 0.0051\n",
      "Epoch 77/100\n",
      "12/12 [==============================] - 2s 153ms/step - loss: 1.3568e-04 - val_loss: 7.1092e-04\n",
      "Epoch 78/100\n",
      "12/12 [==============================] - 2s 153ms/step - loss: 2.7631e-04 - val_loss: 7.9899e-04\n",
      "Epoch 79/100\n",
      "12/12 [==============================] - 2s 166ms/step - loss: 2.6186e-04 - val_loss: 0.0012\n",
      "Epoch 80/100\n",
      "12/12 [==============================] - 2s 154ms/step - loss: 1.4498e-04 - val_loss: 8.4930e-04\n",
      "Epoch 81/100\n",
      "12/12 [==============================] - 3s 211ms/step - loss: 2.9202e-04 - val_loss: 0.0011\n",
      "Epoch 82/100\n",
      "12/12 [==============================] - 2s 186ms/step - loss: 1.6443e-04 - val_loss: 0.0016\n",
      "Epoch 83/100\n",
      "12/12 [==============================] - 2s 201ms/step - loss: 2.1039e-04 - val_loss: 7.2370e-04\n",
      "Epoch 84/100\n",
      "12/12 [==============================] - 2s 149ms/step - loss: 2.2624e-04 - val_loss: 7.0100e-04\n",
      "Epoch 85/100\n",
      "12/12 [==============================] - 2s 191ms/step - loss: 1.0375e-04 - val_loss: 0.0076\n",
      "Epoch 86/100\n",
      "12/12 [==============================] - 2s 192ms/step - loss: 2.4916e-04 - val_loss: 6.0372e-04\n",
      "Epoch 87/100\n",
      "12/12 [==============================] - 2s 202ms/step - loss: 2.6845e-04 - val_loss: 9.8376e-04\n",
      "Epoch 88/100\n",
      "12/12 [==============================] - 2s 157ms/step - loss: 1.7733e-04 - val_loss: 0.0016\n",
      "Epoch 89/100\n",
      "12/12 [==============================] - 2s 148ms/step - loss: 2.2108e-04 - val_loss: 6.2183e-04\n",
      "Epoch 90/100\n",
      "12/12 [==============================] - 2s 149ms/step - loss: 2.3902e-04 - val_loss: 0.0010\n",
      "Epoch 91/100\n",
      "12/12 [==============================] - 2s 149ms/step - loss: 2.0858e-04 - val_loss: 0.0018\n",
      "Epoch 92/100\n",
      "12/12 [==============================] - 2s 148ms/step - loss: 2.0791e-04 - val_loss: 0.0012\n",
      "Epoch 93/100\n",
      "12/12 [==============================] - 2s 148ms/step - loss: 2.2046e-04 - val_loss: 6.9123e-04\n",
      "Epoch 94/100\n",
      "12/12 [==============================] - 2s 150ms/step - loss: 8.3806e-05 - val_loss: 9.5992e-04\n",
      "Epoch 95/100\n",
      "12/12 [==============================] - 2s 149ms/step - loss: 2.1917e-04 - val_loss: 0.0017\n",
      "Epoch 96/100\n",
      "12/12 [==============================] - 2s 151ms/step - loss: 2.5866e-04 - val_loss: 7.3556e-04\n",
      "Epoch 97/100\n",
      "12/12 [==============================] - 2s 149ms/step - loss: 1.7884e-04 - val_loss: 0.0020\n",
      "Epoch 98/100\n",
      "12/12 [==============================] - 2s 150ms/step - loss: 1.8314e-04 - val_loss: 0.0012\n",
      "Epoch 99/100\n",
      "12/12 [==============================] - 2s 152ms/step - loss: 1.7443e-04 - val_loss: 0.0020\n",
      "Epoch 100/100\n",
      "12/12 [==============================] - 2s 159ms/step - loss: 1.7759e-04 - val_loss: 0.0029\n",
      "23/23 [==============================] - 1s 18ms/step\n",
      "11/11 [==============================] - 0s 20ms/step\n",
      "Model: \"sequential_17\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_21 (LSTM)              (None, 100, 50)           10400     \n",
      "                                                                 \n",
      " lstm_22 (LSTM)              (None, 50)                20200     \n",
      "                                                                 \n",
      " dense_17 (Dense)            (None, 1)                 51        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 30,651\n",
      "Trainable params: 30,651\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Aviral\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\optimizers\\optimizer_v2\\rmsprop.py:140: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 4s 167ms/step - loss: 0.0504 - val_loss: 0.2951\n",
      "Epoch 2/100\n",
      "12/12 [==============================] - 2s 128ms/step - loss: 0.0334 - val_loss: 0.2146\n",
      "Epoch 3/100\n",
      "12/12 [==============================] - 2s 131ms/step - loss: 0.0211 - val_loss: 0.1390\n",
      "Epoch 4/100\n",
      "12/12 [==============================] - 2s 134ms/step - loss: 0.0111 - val_loss: 0.0733\n",
      "Epoch 5/100\n",
      "12/12 [==============================] - 2s 137ms/step - loss: 0.0052 - val_loss: 0.0303\n",
      "Epoch 6/100\n",
      "12/12 [==============================] - 2s 132ms/step - loss: 0.0031 - val_loss: 0.0118\n",
      "Epoch 7/100\n",
      "12/12 [==============================] - 2s 131ms/step - loss: 0.0022 - val_loss: 0.0055\n",
      "Epoch 8/100\n",
      "12/12 [==============================] - 2s 132ms/step - loss: 0.0014 - val_loss: 0.0039\n",
      "Epoch 9/100\n",
      "12/12 [==============================] - 2s 133ms/step - loss: 8.6960e-04 - val_loss: 0.0045\n",
      "Epoch 10/100\n",
      "12/12 [==============================] - 2s 131ms/step - loss: 6.4522e-04 - val_loss: 0.0081\n",
      "Epoch 11/100\n",
      "12/12 [==============================] - 2s 135ms/step - loss: 5.6700e-04 - val_loss: 0.0071\n",
      "Epoch 12/100\n",
      "12/12 [==============================] - 2s 131ms/step - loss: 5.3821e-04 - val_loss: 0.0124\n",
      "Epoch 13/100\n",
      "12/12 [==============================] - 2s 131ms/step - loss: 5.3785e-04 - val_loss: 0.0088\n",
      "Epoch 14/100\n",
      "12/12 [==============================] - 2s 132ms/step - loss: 5.1056e-04 - val_loss: 0.0046\n",
      "Epoch 15/100\n",
      "12/12 [==============================] - 2s 134ms/step - loss: 5.3652e-04 - val_loss: 0.0059\n",
      "Epoch 16/100\n",
      "12/12 [==============================] - 2s 132ms/step - loss: 5.1756e-04 - val_loss: 0.0123\n",
      "Epoch 17/100\n",
      "12/12 [==============================] - 2s 132ms/step - loss: 5.1277e-04 - val_loss: 0.0079\n",
      "Epoch 18/100\n",
      "12/12 [==============================] - 2s 134ms/step - loss: 4.9669e-04 - val_loss: 0.0052\n",
      "Epoch 19/100\n",
      "12/12 [==============================] - 2s 133ms/step - loss: 4.7640e-04 - val_loss: 0.0111\n",
      "Epoch 20/100\n",
      "12/12 [==============================] - 2s 155ms/step - loss: 5.1717e-04 - val_loss: 0.0112\n",
      "Epoch 21/100\n",
      "12/12 [==============================] - 2s 159ms/step - loss: 5.0488e-04 - val_loss: 0.0049\n",
      "Epoch 22/100\n",
      "12/12 [==============================] - 2s 129ms/step - loss: 4.6348e-04 - val_loss: 0.0037\n",
      "Epoch 23/100\n",
      "12/12 [==============================] - 2s 128ms/step - loss: 5.0884e-04 - val_loss: 0.0073\n",
      "Epoch 24/100\n",
      "12/12 [==============================] - 2s 129ms/step - loss: 4.7201e-04 - val_loss: 0.0042\n",
      "Epoch 25/100\n",
      "12/12 [==============================] - 2s 130ms/step - loss: 4.5679e-04 - val_loss: 0.0069\n",
      "Epoch 26/100\n",
      "12/12 [==============================] - 2s 181ms/step - loss: 4.6787e-04 - val_loss: 0.0040\n",
      "Epoch 27/100\n",
      "12/12 [==============================] - 2s 180ms/step - loss: 4.4981e-04 - val_loss: 0.0033\n",
      "Epoch 28/100\n",
      "12/12 [==============================] - 2s 141ms/step - loss: 4.3945e-04 - val_loss: 0.0032\n",
      "Epoch 29/100\n",
      "12/12 [==============================] - 2s 127ms/step - loss: 4.7253e-04 - val_loss: 0.0034\n",
      "Epoch 30/100\n",
      "12/12 [==============================] - 2s 127ms/step - loss: 4.3904e-04 - val_loss: 0.0037\n",
      "Epoch 31/100\n",
      "12/12 [==============================] - 2s 126ms/step - loss: 4.4669e-04 - val_loss: 0.0035\n",
      "Epoch 32/100\n",
      "12/12 [==============================] - 2s 126ms/step - loss: 4.3464e-04 - val_loss: 0.0078\n",
      "Epoch 33/100\n",
      "12/12 [==============================] - 2s 130ms/step - loss: 4.4578e-04 - val_loss: 0.0035\n",
      "Epoch 34/100\n",
      "12/12 [==============================] - 2s 142ms/step - loss: 4.3829e-04 - val_loss: 0.0033\n",
      "Epoch 35/100\n",
      "12/12 [==============================] - 2s 140ms/step - loss: 4.1492e-04 - val_loss: 0.0050\n",
      "Epoch 36/100\n",
      "12/12 [==============================] - 2s 134ms/step - loss: 4.1560e-04 - val_loss: 0.0037\n",
      "Epoch 37/100\n",
      "12/12 [==============================] - 2s 162ms/step - loss: 4.3721e-04 - val_loss: 0.0030\n",
      "Epoch 38/100\n",
      "12/12 [==============================] - 2s 171ms/step - loss: 4.1361e-04 - val_loss: 0.0029\n",
      "Epoch 39/100\n",
      "12/12 [==============================] - 2s 174ms/step - loss: 4.0360e-04 - val_loss: 0.0040\n",
      "Epoch 40/100\n",
      "12/12 [==============================] - 2s 174ms/step - loss: 4.1649e-04 - val_loss: 0.0047\n",
      "Epoch 41/100\n",
      "12/12 [==============================] - 2s 131ms/step - loss: 3.9467e-04 - val_loss: 0.0025\n",
      "Epoch 42/100\n",
      "12/12 [==============================] - 2s 128ms/step - loss: 3.9414e-04 - val_loss: 0.0062\n",
      "Epoch 43/100\n",
      "12/12 [==============================] - 2s 133ms/step - loss: 4.0114e-04 - val_loss: 0.0036\n",
      "Epoch 44/100\n",
      "12/12 [==============================] - 2s 134ms/step - loss: 3.8545e-04 - val_loss: 0.0026\n",
      "Epoch 45/100\n",
      "12/12 [==============================] - 2s 135ms/step - loss: 4.0309e-04 - val_loss: 0.0034\n",
      "Epoch 46/100\n",
      "12/12 [==============================] - 2s 128ms/step - loss: 3.7914e-04 - val_loss: 0.0026\n",
      "Epoch 47/100\n",
      "12/12 [==============================] - 2s 130ms/step - loss: 3.8027e-04 - val_loss: 0.0026\n",
      "Epoch 48/100\n",
      "12/12 [==============================] - 2s 127ms/step - loss: 3.9336e-04 - val_loss: 0.0034\n",
      "Epoch 49/100\n",
      "12/12 [==============================] - 2s 129ms/step - loss: 3.6670e-04 - val_loss: 0.0023\n",
      "Epoch 50/100\n",
      "12/12 [==============================] - 2s 128ms/step - loss: 4.0061e-04 - val_loss: 0.0021\n",
      "Epoch 51/100\n",
      "12/12 [==============================] - 2s 130ms/step - loss: 3.6290e-04 - val_loss: 0.0033\n",
      "Epoch 52/100\n",
      "12/12 [==============================] - 2s 128ms/step - loss: 3.8307e-04 - val_loss: 0.0026\n",
      "Epoch 53/100\n",
      "12/12 [==============================] - 2s 127ms/step - loss: 3.6507e-04 - val_loss: 0.0023\n",
      "Epoch 54/100\n",
      "12/12 [==============================] - 2s 127ms/step - loss: 3.6450e-04 - val_loss: 0.0025\n",
      "Epoch 55/100\n",
      "12/12 [==============================] - 2s 129ms/step - loss: 3.5924e-04 - val_loss: 0.0023\n",
      "Epoch 56/100\n",
      "12/12 [==============================] - 2s 127ms/step - loss: 3.6482e-04 - val_loss: 0.0023\n",
      "Epoch 57/100\n",
      "12/12 [==============================] - 2s 131ms/step - loss: 3.4480e-04 - val_loss: 0.0020\n",
      "Epoch 58/100\n",
      "12/12 [==============================] - 2s 128ms/step - loss: 3.6598e-04 - val_loss: 0.0018\n",
      "Epoch 59/100\n",
      "12/12 [==============================] - 2s 129ms/step - loss: 3.5323e-04 - val_loss: 0.0031\n",
      "Epoch 60/100\n",
      "12/12 [==============================] - 2s 129ms/step - loss: 3.4708e-04 - val_loss: 0.0019\n",
      "Epoch 61/100\n",
      "12/12 [==============================] - 2s 129ms/step - loss: 3.4219e-04 - val_loss: 0.0031\n",
      "Epoch 62/100\n",
      "12/12 [==============================] - 2s 129ms/step - loss: 3.5063e-04 - val_loss: 0.0026\n",
      "Epoch 63/100\n",
      "12/12 [==============================] - 2s 128ms/step - loss: 3.3503e-04 - val_loss: 0.0019\n",
      "Epoch 64/100\n",
      "12/12 [==============================] - 2s 129ms/step - loss: 3.3814e-04 - val_loss: 0.0018\n",
      "Epoch 65/100\n",
      "12/12 [==============================] - 2s 128ms/step - loss: 3.2891e-04 - val_loss: 0.0019\n",
      "Epoch 66/100\n",
      "12/12 [==============================] - 2s 134ms/step - loss: 3.3552e-04 - val_loss: 0.0017\n",
      "Epoch 67/100\n",
      "12/12 [==============================] - 2s 129ms/step - loss: 3.3797e-04 - val_loss: 0.0016\n",
      "Epoch 68/100\n",
      "12/12 [==============================] - 2s 128ms/step - loss: 3.2131e-04 - val_loss: 0.0016\n",
      "Epoch 69/100\n",
      "12/12 [==============================] - 2s 127ms/step - loss: 3.3309e-04 - val_loss: 0.0016\n",
      "Epoch 70/100\n",
      "12/12 [==============================] - 2s 128ms/step - loss: 3.2168e-04 - val_loss: 0.0019\n",
      "Epoch 71/100\n",
      "12/12 [==============================] - 2s 129ms/step - loss: 3.3074e-04 - val_loss: 0.0019\n",
      "Epoch 72/100\n",
      "12/12 [==============================] - 2s 128ms/step - loss: 3.3050e-04 - val_loss: 0.0016\n",
      "Epoch 73/100\n",
      "12/12 [==============================] - 2s 129ms/step - loss: 3.1842e-04 - val_loss: 0.0024\n",
      "Epoch 74/100\n",
      "12/12 [==============================] - 2s 128ms/step - loss: 3.1636e-04 - val_loss: 0.0016\n",
      "Epoch 75/100\n",
      "12/12 [==============================] - 2s 127ms/step - loss: 3.0997e-04 - val_loss: 0.0024\n",
      "Epoch 76/100\n",
      "12/12 [==============================] - 2s 128ms/step - loss: 3.1316e-04 - val_loss: 0.0017\n",
      "Epoch 77/100\n",
      "12/12 [==============================] - 2s 129ms/step - loss: 3.0387e-04 - val_loss: 0.0016\n",
      "Epoch 78/100\n",
      "12/12 [==============================] - 2s 129ms/step - loss: 3.1033e-04 - val_loss: 0.0021\n",
      "Epoch 79/100\n",
      "12/12 [==============================] - 2s 133ms/step - loss: 3.1442e-04 - val_loss: 0.0022\n",
      "Epoch 80/100\n",
      "12/12 [==============================] - 2s 130ms/step - loss: 3.1584e-04 - val_loss: 0.0017\n",
      "Epoch 81/100\n",
      "12/12 [==============================] - 2s 128ms/step - loss: 3.0364e-04 - val_loss: 0.0014\n",
      "Epoch 82/100\n",
      "12/12 [==============================] - 2s 128ms/step - loss: 3.1349e-04 - val_loss: 0.0014\n",
      "Epoch 83/100\n",
      "12/12 [==============================] - 2s 127ms/step - loss: 3.0405e-04 - val_loss: 0.0014\n",
      "Epoch 84/100\n",
      "12/12 [==============================] - 2s 127ms/step - loss: 2.9386e-04 - val_loss: 0.0021\n",
      "Epoch 85/100\n",
      "12/12 [==============================] - 2s 131ms/step - loss: 3.0894e-04 - val_loss: 0.0014\n",
      "Epoch 86/100\n",
      "12/12 [==============================] - 2s 128ms/step - loss: 2.9919e-04 - val_loss: 0.0020\n",
      "Epoch 87/100\n",
      "12/12 [==============================] - 2s 129ms/step - loss: 2.9078e-04 - val_loss: 0.0014\n",
      "Epoch 88/100\n",
      "12/12 [==============================] - 2s 130ms/step - loss: 2.9099e-04 - val_loss: 0.0013\n",
      "Epoch 89/100\n",
      "12/12 [==============================] - 2s 129ms/step - loss: 2.8426e-04 - val_loss: 0.0019\n",
      "Epoch 90/100\n",
      "12/12 [==============================] - 2s 129ms/step - loss: 2.9842e-04 - val_loss: 0.0013\n",
      "Epoch 91/100\n",
      "12/12 [==============================] - 2s 128ms/step - loss: 2.7704e-04 - val_loss: 0.0012\n",
      "Epoch 92/100\n",
      "12/12 [==============================] - 2s 129ms/step - loss: 2.8078e-04 - val_loss: 0.0012\n",
      "Epoch 93/100\n",
      "12/12 [==============================] - 2s 129ms/step - loss: 2.7828e-04 - val_loss: 0.0012\n",
      "Epoch 94/100\n",
      "12/12 [==============================] - 2s 127ms/step - loss: 2.7498e-04 - val_loss: 0.0016\n",
      "Epoch 95/100\n",
      "12/12 [==============================] - 2s 127ms/step - loss: 2.8234e-04 - val_loss: 0.0018\n",
      "Epoch 96/100\n",
      "12/12 [==============================] - 2s 129ms/step - loss: 2.8388e-04 - val_loss: 0.0018\n",
      "Epoch 97/100\n",
      "12/12 [==============================] - 2s 129ms/step - loss: 2.7662e-04 - val_loss: 0.0013\n",
      "Epoch 98/100\n",
      "12/12 [==============================] - 2s 129ms/step - loss: 2.6905e-04 - val_loss: 0.0023\n",
      "Epoch 99/100\n",
      "12/12 [==============================] - 2s 128ms/step - loss: 2.8708e-04 - val_loss: 0.0012\n",
      "Epoch 100/100\n",
      "12/12 [==============================] - 2s 128ms/step - loss: 2.7792e-04 - val_loss: 0.0014\n",
      "23/23 [==============================] - 1s 16ms/step\n",
      "11/11 [==============================] - 0s 17ms/step\n",
      "Best Model for AAPL is \n",
      "Best learning rate is 0.001\n",
      "Minimum mean squared error is 139.04828351739283\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import math\n",
    "from sklearn.metrics import mean_squared_error\n",
    "learn_rates = [0.01,0.001,0.0001]\n",
    "c=10000\n",
    "learn_rate=0\n",
    "model_name=\"\"\n",
    "for model_item in models:\n",
    "    for lr in learn_rates:\n",
    "        model = model_item[1](lr)\n",
    "        model.summary()\n",
    "        model.fit(X_train,y_train,validation_data=(X_test,ytest),epochs=100,batch_size=64,verbose=1)\n",
    "\n",
    "        train_predict=model.predict(X_train)\n",
    "        test_predict=model.predict(X_test)\n",
    "        train_predict=scaler.inverse_transform(train_predict)\n",
    "        test_predict=scaler.inverse_transform(test_predict)\n",
    "        if(c>math.sqrt(mean_squared_error(y_train,train_predict))):\n",
    "            c=math.sqrt(mean_squared_error(y_train,train_predict))\n",
    "            learn_rate=lr\n",
    "            model_name=model_item[0]\n",
    "\n",
    "print(\"Best Model for AAPL is \".format(model_name))\n",
    "print(\"Best learning rate is {}\".format(learn_rate))\n",
    "print(\"Minimum mean squared error is {}\".format(c))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "3f104240",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stacked\n"
     ]
    }
   ],
   "source": [
    "print(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "44656cb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_17\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_21 (LSTM)              (None, 100, 50)           10400     \n",
      "                                                                 \n",
      " lstm_22 (LSTM)              (None, 50)                20200     \n",
      "                                                                 \n",
      " dense_17 (Dense)            (None, 1)                 51        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 30,651\n",
      "Trainable params: 30,651\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "42030c22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "12/12 [==============================] - 2s 126ms/step - loss: 2.6566e-04 - val_loss: 0.0012\n",
      "Epoch 2/100\n",
      "12/12 [==============================] - 1s 121ms/step - loss: 2.7708e-04 - val_loss: 0.0011\n",
      "Epoch 3/100\n",
      "12/12 [==============================] - 2s 131ms/step - loss: 2.8183e-04 - val_loss: 0.0017\n",
      "Epoch 4/100\n",
      "12/12 [==============================] - 2s 145ms/step - loss: 2.7640e-04 - val_loss: 0.0014\n",
      "Epoch 5/100\n",
      "12/12 [==============================] - 2s 151ms/step - loss: 2.5943e-04 - val_loss: 0.0017\n",
      "Epoch 6/100\n",
      "12/12 [==============================] - 2s 167ms/step - loss: 2.6442e-04 - val_loss: 0.0011\n",
      "Epoch 7/100\n",
      "12/12 [==============================] - 2s 180ms/step - loss: 2.7579e-04 - val_loss: 0.0014\n",
      "Epoch 8/100\n",
      "12/12 [==============================] - 2s 147ms/step - loss: 2.5781e-04 - val_loss: 0.0021\n",
      "Epoch 9/100\n",
      "12/12 [==============================] - 2s 139ms/step - loss: 2.7738e-04 - val_loss: 0.0012\n",
      "Epoch 10/100\n",
      "12/12 [==============================] - 2s 130ms/step - loss: 2.6982e-04 - val_loss: 0.0012\n",
      "Epoch 11/100\n",
      "12/12 [==============================] - 2s 129ms/step - loss: 2.5614e-04 - val_loss: 0.0019\n",
      "Epoch 12/100\n",
      "12/12 [==============================] - 1s 121ms/step - loss: 2.7668e-04 - val_loss: 0.0011\n",
      "Epoch 13/100\n",
      "12/12 [==============================] - 2s 175ms/step - loss: 2.5797e-04 - val_loss: 0.0011\n",
      "Epoch 14/100\n",
      "12/12 [==============================] - 2s 172ms/step - loss: 2.5549e-04 - val_loss: 0.0011\n",
      "Epoch 15/100\n",
      "12/12 [==============================] - 2s 138ms/step - loss: 2.7175e-04 - val_loss: 0.0013\n",
      "Epoch 16/100\n",
      "12/12 [==============================] - 2s 134ms/step - loss: 2.4823e-04 - val_loss: 0.0011\n",
      "Epoch 17/100\n",
      "12/12 [==============================] - 2s 136ms/step - loss: 2.5985e-04 - val_loss: 0.0012\n",
      "Epoch 18/100\n",
      "12/12 [==============================] - 2s 141ms/step - loss: 2.5367e-04 - val_loss: 0.0019\n",
      "Epoch 19/100\n",
      "12/12 [==============================] - 2s 152ms/step - loss: 2.5402e-04 - val_loss: 0.0012\n",
      "Epoch 20/100\n",
      "12/12 [==============================] - 2s 159ms/step - loss: 2.4188e-04 - val_loss: 0.0017\n",
      "Epoch 21/100\n",
      "12/12 [==============================] - 2s 171ms/step - loss: 2.7873e-04 - val_loss: 0.0013\n",
      "Epoch 22/100\n",
      "12/12 [==============================] - 2s 145ms/step - loss: 2.4728e-04 - val_loss: 0.0011\n",
      "Epoch 23/100\n",
      "12/12 [==============================] - 2s 136ms/step - loss: 2.5089e-04 - val_loss: 0.0019\n",
      "Epoch 24/100\n",
      "12/12 [==============================] - 2s 149ms/step - loss: 2.4894e-04 - val_loss: 0.0013\n",
      "Epoch 25/100\n",
      "12/12 [==============================] - 2s 145ms/step - loss: 2.6099e-04 - val_loss: 0.0011\n",
      "Epoch 26/100\n",
      "12/12 [==============================] - 1s 125ms/step - loss: 2.5114e-04 - val_loss: 0.0011\n",
      "Epoch 27/100\n",
      "12/12 [==============================] - 2s 134ms/step - loss: 2.3852e-04 - val_loss: 0.0011\n",
      "Epoch 28/100\n",
      "12/12 [==============================] - 2s 134ms/step - loss: 2.4635e-04 - val_loss: 0.0021\n",
      "Epoch 29/100\n",
      "12/12 [==============================] - 2s 137ms/step - loss: 2.5560e-04 - val_loss: 0.0011\n",
      "Epoch 30/100\n",
      "12/12 [==============================] - 2s 134ms/step - loss: 2.4085e-04 - val_loss: 0.0012\n",
      "Epoch 31/100\n",
      "12/12 [==============================] - 2s 131ms/step - loss: 2.3986e-04 - val_loss: 0.0015\n",
      "Epoch 32/100\n",
      "12/12 [==============================] - 2s 146ms/step - loss: 2.4188e-04 - val_loss: 0.0016\n",
      "Epoch 33/100\n",
      "12/12 [==============================] - 2s 148ms/step - loss: 2.3622e-04 - val_loss: 0.0010\n",
      "Epoch 34/100\n",
      "12/12 [==============================] - 2s 174ms/step - loss: 2.3566e-04 - val_loss: 0.0018\n",
      "Epoch 35/100\n",
      "12/12 [==============================] - 2s 163ms/step - loss: 2.2726e-04 - val_loss: 0.0012\n",
      "Epoch 36/100\n",
      "12/12 [==============================] - 2s 156ms/step - loss: 2.3559e-04 - val_loss: 0.0010\n",
      "Epoch 37/100\n",
      "12/12 [==============================] - 2s 161ms/step - loss: 2.3324e-04 - val_loss: 0.0011\n",
      "Epoch 38/100\n",
      "12/12 [==============================] - 2s 152ms/step - loss: 2.2939e-04 - val_loss: 0.0014\n",
      "Epoch 39/100\n",
      " 4/12 [=========>....................] - ETA: 1s - loss: 2.2999e-04"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Aviral\\Downloads\\Stock-MArket-Forecasting-master\\Stock-MArket-Forecasting-master\\stock3.ipynb Cell 30\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Aviral/Downloads/Stock-MArket-Forecasting-master/Stock-MArket-Forecasting-master/stock3.ipynb#X32sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m model\u001b[39m.\u001b[39;49mfit(X_train,y_train,validation_data\u001b[39m=\u001b[39;49m(X_test,ytest),epochs\u001b[39m=\u001b[39;49m\u001b[39m100\u001b[39;49m,batch_size\u001b[39m=\u001b[39;49m\u001b[39m64\u001b[39;49m,verbose\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training.py:1564\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1556\u001b[0m \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mexperimental\u001b[39m.\u001b[39mTrace(\n\u001b[0;32m   1557\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   1558\u001b[0m     epoch_num\u001b[39m=\u001b[39mepoch,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1561\u001b[0m     _r\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m,\n\u001b[0;32m   1562\u001b[0m ):\n\u001b[0;32m   1563\u001b[0m     callbacks\u001b[39m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m-> 1564\u001b[0m     tmp_logs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_function(iterator)\n\u001b[0;32m   1565\u001b[0m     \u001b[39mif\u001b[39;00m data_handler\u001b[39m.\u001b[39mshould_sync:\n\u001b[0;32m   1566\u001b[0m         context\u001b[39m.\u001b[39masync_wait()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\tensorflow\\python\\eager\\def_function.py:915\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    912\u001b[0m compiler \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mxla\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mnonXla\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    914\u001b[0m \u001b[39mwith\u001b[39;00m OptionalXlaContext(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 915\u001b[0m   result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n\u001b[0;32m    917\u001b[0m new_tracing_count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    918\u001b[0m without_tracing \u001b[39m=\u001b[39m (tracing_count \u001b[39m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\tensorflow\\python\\eager\\def_function.py:947\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    944\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n\u001b[0;32m    945\u001b[0m   \u001b[39m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[0;32m    946\u001b[0m   \u001b[39m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[1;32m--> 947\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_stateless_fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)  \u001b[39m# pylint: disable=not-callable\u001b[39;00m\n\u001b[0;32m    948\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_stateful_fn \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    949\u001b[0m   \u001b[39m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[0;32m    950\u001b[0m   \u001b[39m# in parallel.\u001b[39;00m\n\u001b[0;32m    951\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\tensorflow\\python\\eager\\function.py:2496\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2493\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[0;32m   2494\u001b[0m   (graph_function,\n\u001b[0;32m   2495\u001b[0m    filtered_flat_args) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[1;32m-> 2496\u001b[0m \u001b[39mreturn\u001b[39;00m graph_function\u001b[39m.\u001b[39;49m_call_flat(\n\u001b[0;32m   2497\u001b[0m     filtered_flat_args, captured_inputs\u001b[39m=\u001b[39;49mgraph_function\u001b[39m.\u001b[39;49mcaptured_inputs)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\tensorflow\\python\\eager\\function.py:1862\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1858\u001b[0m possible_gradient_type \u001b[39m=\u001b[39m gradients_util\u001b[39m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1859\u001b[0m \u001b[39mif\u001b[39;00m (possible_gradient_type \u001b[39m==\u001b[39m gradients_util\u001b[39m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1860\u001b[0m     \u001b[39mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1861\u001b[0m   \u001b[39m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1862\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_call_outputs(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_inference_function\u001b[39m.\u001b[39;49mcall(\n\u001b[0;32m   1863\u001b[0m       ctx, args, cancellation_manager\u001b[39m=\u001b[39;49mcancellation_manager))\n\u001b[0;32m   1864\u001b[0m forward_backward \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1865\u001b[0m     args,\n\u001b[0;32m   1866\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1867\u001b[0m     executing_eagerly)\n\u001b[0;32m   1868\u001b[0m forward_function, args_with_tangents \u001b[39m=\u001b[39m forward_backward\u001b[39m.\u001b[39mforward()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\tensorflow\\python\\eager\\function.py:499\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    497\u001b[0m \u001b[39mwith\u001b[39;00m _InterpolateFunctionError(\u001b[39mself\u001b[39m):\n\u001b[0;32m    498\u001b[0m   \u001b[39mif\u001b[39;00m cancellation_manager \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 499\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39;49mexecute(\n\u001b[0;32m    500\u001b[0m         \u001b[39mstr\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msignature\u001b[39m.\u001b[39;49mname),\n\u001b[0;32m    501\u001b[0m         num_outputs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_num_outputs,\n\u001b[0;32m    502\u001b[0m         inputs\u001b[39m=\u001b[39;49margs,\n\u001b[0;32m    503\u001b[0m         attrs\u001b[39m=\u001b[39;49mattrs,\n\u001b[0;32m    504\u001b[0m         ctx\u001b[39m=\u001b[39;49mctx)\n\u001b[0;32m    505\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    506\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m    507\u001b[0m         \u001b[39mstr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msignature\u001b[39m.\u001b[39mname),\n\u001b[0;32m    508\u001b[0m         num_outputs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    511\u001b[0m         ctx\u001b[39m=\u001b[39mctx,\n\u001b[0;32m    512\u001b[0m         cancellation_manager\u001b[39m=\u001b[39mcancellation_manager)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\tensorflow\\python\\eager\\execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 54\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_Execute(ctx\u001b[39m.\u001b[39;49m_handle, device_name, op_name,\n\u001b[0;32m     55\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[0;32m     56\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     57\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#model.fit(X_train,y_train,validation_data=(X_test,ytest),epochs=100,batch_size=64,verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b985d29",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1d92af6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.10.0'"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "1d107fc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/23 [==============================] - 0s 19ms/step\n",
      "11/11 [==============================] - 0s 19ms/step\n"
     ]
    }
   ],
   "source": [
    "train_predict=model.predict(X_train)\n",
    "test_predict=model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "6bd5caa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_predict=scaler.inverse_transform(train_predict)\n",
    "test_predict=scaler.inverse_transform(test_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "ae388398",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "142.19215869758025"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "from sklearn.metrics import mean_squared_error\n",
    "math.sqrt(mean_squared_error(y_train,train_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "a92e58f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "235.51101247487108"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "math.sqrt(mean_squared_error(ytest,test_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "0f1a7d1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "look_back=100\n",
    "trainPredictPlot = numpy.empty_like(df1)\n",
    "trainPredictPlot[:, :] = np.nan\n",
    "trainPredictPlot[look_back:len(train_predict)+look_back, :] = train_predict\n",
    "# shift test predictions for plotting\n",
    "testPredictPlot = numpy.empty_like(df1)\n",
    "testPredictPlot[:, :] = numpy.nan\n",
    "testPredictPlot[len(train_predict)+(look_back*2)+1:len(df1)-1, :] = test_predict\n",
    "# plot baseline and predictions\n",
    "plt.plot(scaler.inverse_transform(df1))\n",
    "plt.plot(trainPredictPlot)\n",
    "plt.plot(testPredictPlot)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1a65b88",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "vscode": {
   "interpreter": {
    "hash": "369f2c481f4da34e4445cda3fffd2e751bd1c4d706f27375911949ba6bb62e1c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
